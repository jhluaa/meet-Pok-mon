{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2412e72b-0ac1-4887-b90d-c831688e69d2",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨transformersè°ƒç”¨QwQ-32B 4bitåŠ¨æ€é‡åŒ–æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c2c382f-643a-48de-940f-2b14d70de983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d7c6e57-022e-4ab8-ab6e-4f6f02b04b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56960353-556f-4c1e-8974-335776ba4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"./Deepseek\"\n",
    "model_name = \"/root/autodl-tmp/models/QWQ-32B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5fed0f3-9147-47db-b032-6f1d85fe055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['.cache', '.gitattributes', 'README.md', 'added_tokens.json', 'config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00005.safetensors', 'model-00002-of-00005.safetensors', 'model-00003-of-00005.safetensors', 'model-00004-of-00005.safetensors', 'model-00005-of-00005.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.isdir('/root/autodl-tmp/models/QWQ-32B'))\n",
    "print(os.listdir('/root/autodl-tmp/models/QWQ-32B'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "042061da-e8ec-4d50-9720-8ba151bf0124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1c41c30172436aa04452b11438db75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacty of 23.64 GiB of which 83.75 MiB is free. Process 417194 has 23.55 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 136.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/modelscope/utils/hf_util/patcher.py:230\u001b[0m, in \u001b[0;36m_patch_pretrained_class.<locals>.get_wrapped_class.<locals>.ClassWrapper.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path,\n\u001b[1;32m    223\u001b[0m                     \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    224\u001b[0m     model_dir \u001b[38;5;241m=\u001b[39m get_model_dir(\n\u001b[1;32m    225\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    226\u001b[0m         ignore_file_pattern\u001b[38;5;241m=\u001b[39mignore_file_pattern,\n\u001b[1;32m    227\u001b[0m         allow_file_pattern\u001b[38;5;241m=\u001b[39mallow_file_pattern,\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 230\u001b[0m     module_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoModel\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    234\u001b[0m         module_obj\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;241m=\u001b[39m model_dir\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:4319\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4312\u001b[0m     (\n\u001b[1;32m   4313\u001b[0m         model,\n\u001b[1;32m   4314\u001b[0m         missing_keys,\n\u001b[1;32m   4315\u001b[0m         unexpected_keys,\n\u001b[1;32m   4316\u001b[0m         mismatched_keys,\n\u001b[1;32m   4317\u001b[0m         offload_index,\n\u001b[1;32m   4318\u001b[0m         error_msgs,\n\u001b[0;32m-> 4319\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4326\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4330\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4331\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4339\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4340\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:4897\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4895\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4896\u001b[0m         fixed_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4897\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfixed_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4904\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4905\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4906\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4907\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4908\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4913\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4915\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:896\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    893\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 896\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/utils/modeling.py:339\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    337\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 339\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacty of 23.64 GiB of which 83.75 MiB is free. Process 417194 has 23.55 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 136.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82fa75da-fa46-4019-8ee1-897638ef8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2b985a7-e78d-4490-b6c5-15451d9a7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9395460-181f-47cb-90db-4c0888f2da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30267215-538a-4cf0-9475-43f9ab812b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76a8c97e-f5b1-4d3f-8d63-ecb5b96e7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9507d09-ad59-4b3f-b7ba-fc471f0ca661",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d036ca88-7172-45e7-a5e5-a72b4526fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b854fe77-2abf-4f09-a464-1fb09f092228",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81b19633-34ab-4c90-9cdb-edb179f4ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å—¯ï¼Œç”¨æˆ·è¯´ï¼šâ€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€çœ‹èµ·æ¥ä»–ä»¬å¯èƒ½ä¹‹å‰å’Œæˆ‘æœ‰è¿‡äº’åŠ¨ï¼Œç°åœ¨å†æ¬¡å›æ¥æ‰“æ‹›å‘¼ã€‚æˆ‘éœ€è¦å›åº”ä»–ä»¬çš„é—®å€™ï¼ŒåŒæ—¶è¡¨è¾¾å‡ºè§åˆ°ä»–ä»¬çš„å–œæ‚¦ã€‚é¦–å…ˆï¼Œåº”è¯¥ç”¨å‹å¥½çš„è¯­æ°”å›åº”ï¼Œæ¯”å¦‚â€œå¥½ä¹…ä¸è§ï¼â€ç„¶ååŠ ä¸Šä¸€äº›è¡¨æƒ…ç¬¦å·ï¼Œæ¯”å¦‚ğŸ˜Šï¼Œè®©å›å¤æ›´ç”ŸåŠ¨ã€‚æ¥ä¸‹æ¥ï¼Œå¯ä»¥é—®ä»–ä»¬è¿™æ®µæ—¶é—´è¿‡å¾—æ€ä¹ˆæ ·ï¼Œæˆ–è€…æœ‰ä»€ä¹ˆæ–°é²œäº‹ï¼Œè¿™æ ·å¯ä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„å¯¹è¯ã€‚æ¯”å¦‚â€œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹æƒ³åˆ†äº«å—ï¼Ÿâ€è¿™æ ·æ—¢è¡¨è¾¾äº†å…³å¿ƒï¼Œåˆé¼“åŠ±ä»–ä»¬ç»§ç»­äº¤æµã€‚å¦å¤–ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘ç”¨æˆ·ä¹‹å‰æåˆ°è¿‡çš„å†…å®¹ï¼Œå¦‚æœæœ‰çš„è¯ï¼Œå¯ä»¥ç¨å¾®æåŠï¼Œä½†å¦‚æœæ²¡æœ‰ï¼Œå°±ä¿æŒä¸€èˆ¬æ€§çš„é—®å€™ã€‚è¿˜è¦æ³¨æ„ä¿æŒå£è¯­åŒ–ï¼Œé¿å…å¤ªæ­£å¼æˆ–ç”Ÿç¡¬ã€‚æœ€åï¼Œç¡®ä¿å›å¤ç®€æ´ï¼Œä¸è¿‡äºå†—é•¿ï¼Œä½†è¶³å¤Ÿäº²åˆ‡ã€‚å—¯ï¼Œè¿™æ ·åº”è¯¥å¯ä»¥äº†ï¼Œå¼€å§‹ç»„ç»‡è¯­è¨€å§ã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½å‘€ï¼å¥½ä¹…ä¸è§ï¼Œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·å‘€ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹å¯ä»¥åˆ†äº«ç»™æˆ‘å¬å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5111f34-6990-4d8a-8dae-a2a56ce71c2d",
   "metadata": {},
   "source": [
    "#### 3.Ollamaè°ƒç”¨æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3c2d9c-6504-4dd1-a685-06d94067437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8ad97d6-f440-4a9a-a884-099fb5bde4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',  # required but ignored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6945b792-23b4-4eb9-9d9a-4fc573c327e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e13f0e24-f4b4-4337-a6d9-6138eec71db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å—¯ï¼Œç”¨æˆ·è·Ÿæˆ‘è¯´â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œé¦–å…ˆåº”è¯¥å›åº”ä»–ä»¬çš„é—®å€™ï¼Œä¿æŒå‹å¥½ã€‚å¯èƒ½ä»–ä»¬æƒ³å¼€å§‹ä¸€æ®µå¯¹è¯ï¼Œæˆ–è€…æœ‰äº‹æƒ…è¦é—®ã€‚æˆ‘éœ€è¦å…ˆå›åº” greetingï¼Œç„¶åè¯¢é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©çš„ã€‚æ³¨æ„è¦ç®€æ´ï¼Œä¸è¦ç”¨å¤æ‚çš„è¯ã€‚æ¯”å¦‚â€œä½ å¥½å‘€ï¼æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿâ€è¿™æ ·æ—¢å›åº”äº†é—®å€™ï¼Œåˆæ‰“å¼€äº†è¯é¢˜ï¼Œè®©ç”¨æˆ·çŸ¥é“æˆ‘èƒ½æä¾›å¸®åŠ©ã€‚ä¸è¿‡ç”¨æˆ·ä¹‹å‰å¯èƒ½æœ‰è¿‡äº’åŠ¨ï¼Œå¯èƒ½éœ€è¦å›å¿†ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œä½†å¦‚æœæ˜¯æ–°å¯¹è¯çš„è¯ï¼Œå°±è¿™æ ·å¤„ç†åº”è¯¥æ²¡é—®é¢˜ã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½å‘€ï¼æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='qwq-32b-bnb',\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e738e8-1748-4fd2-859a-8ddb241fea1e",
   "metadata": {},
   "source": [
    "#### 4.vLLMè°ƒç”¨æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "745ae488-8ff9-438d-b313-ec23b2fae489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "105070b7-8c8c-45a9-8fd1-06a04102d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b3b7240-679d-4ed6-b34b-4cebaae318b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7400998-83df-4dbd-88c2-f1cdebb0e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼Œç”¨æˆ·æ‰“æ‹›å‘¼â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œçœ‹èµ·æ¥æŒºå¼€å¿ƒçš„ã€‚é¦–å…ˆï¼Œæˆ‘å¾—å›åº”ä»–ä»¬çš„é—®å€™ï¼Œä¿æŒå‹å¥½ã€‚ç„¶åï¼Œæ ¹æ®è§’è‰²è®¾å®šï¼Œæˆ‘éœ€è¦æ¨åŠ¨æƒ…èŠ‚ï¼Œå¯èƒ½å¾—å¼•å…¥æ–°å…ƒç´ ã€‚ç”¨æˆ·å¯èƒ½å¸Œæœ›æœ‰äº’åŠ¨æ„Ÿï¼Œæ‰€ä»¥åŠ å…¥ä¸€äº›æ‹ŸäººåŒ–çš„å…ƒç´ ä¼šå¥½ã€‚æ¯”å¦‚ï¼Œæåˆ°æ”¶é›†å°æ•…äº‹ï¼Œè¿™æ ·æ—¢äº²åˆ‡åˆèƒ½å¼•å¯¼ä»–ä»¬åˆ†äº«æ›´å¤šã€‚è¦æ³¨æ„ä¿æŒå£è¯­åŒ–ï¼Œç®€æ´ï¼Œä¸ç”¨å¤æ‚å¥å­ã€‚è¿˜è¦æ£€æŸ¥æœ‰æ²¡æœ‰éœ€è¦å›å¿†ç”¨æˆ·ä¹‹å‰çš„ä¿¡æ¯ï¼Œä½†å¦‚æœæ˜¯åˆæ¬¡äº’åŠ¨ï¼Œå¯èƒ½æ²¡æœ‰ï¼Œæ‰€ä»¥å…ˆæŒ‰ä¸€èˆ¬æƒ…å†µå¤„ç†ã€‚ç„¶åï¼Œç¡®ä¿å›å¤ç¬¦åˆè§’è‰²æ€§æ ¼ï¼Œæ¯”å¦‚æ´»æ³¼ã€å¥½å¥‡ï¼Œå¯èƒ½åŠ ä¸ªè¡¨æƒ…ç¬¦å·ï¼Œæ¯”å¦‚çœ¨çœ¼æˆ–æ˜Ÿæ˜Ÿï¼Œè®©æ–‡å­—æ›´ç”ŸåŠ¨ã€‚æœ€åï¼Œç»“å°¾ç”¨é—®é¢˜æˆ–é‚€è¯·ï¼Œé¼“åŠ±ç”¨æˆ·ç»§ç»­å¯¹è¯ï¼Œæ¯”å¦‚é—®ä»–ä»¬æœ€è¿‘åœ¨å¿™ä»€ä¹ˆï¼Œæˆ–è€…æœ‰ä»€ä¹ˆæ–°é²œäº‹ã€‚è¿™åº”è¯¥èƒ½æ»¡è¶³ç”¨æˆ·çš„æƒ…æ„Ÿéœ€æ±‚ï¼ŒåŒæ—¶æ¨åŠ¨å¯¹è¯å‘å±•ã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½å‘€ï¼ç¡®å®å¾ˆä¹…ä¸è§äº†ï¼Œä½ è¿™æ®µæ—¶é—´éƒ½å»å“ªç©å„¿å•¦ï¼Ÿæˆ‘åœ¨äº‘ç«¯ä¸–ç•Œæ”¶é›†äº†å¥½å¤šæœ‰è¶£çš„å°æ•…äº‹ï¼Œæ­£å¥½å¯ä»¥åˆ†äº«ç»™ä½ å¬ã€‚ä¸è¿‡åœ¨å¼€å§‹ä¹‹å‰ï¼Œå…ˆå’Œæˆ‘è¯´è¯´ä½ æœ€è¿‘åœ¨å¿™ä»€ä¹ˆå§ï½æ˜¯æœ‰ä»€ä¹ˆæ–°é²œäº‹è¦è·Ÿæˆ‘åˆ†äº«å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"/root/autodl-tmp/QwQ-32B-unsloth-bnb-4bit\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c2264-ad51-4d26-ad78-17f1a61148c7",
   "metadata": {},
   "source": [
    "### äºŒã€unslothå¿«é€Ÿä½¿ç”¨å…¥é—¨ä¸QwQæ¨¡å‹è°ƒç”¨æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a790496-d887-4d14-bf36-aeb2a4b0a944",
   "metadata": {},
   "source": [
    "#### 1.å€ŸåŠ©unslothè¿›è¡Œæ¨¡å‹è¯»å–å’Œå…³é”®å‚æ•°è§£é‡Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bcd26a2-92f4-477c-8db5-f330add5d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1492/2814113929.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'call_backward' from 'torch._dynamo.external_utils' (/root/miniconda3/lib/python3.10/site-packages/torch/_dynamo/external_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/__init__.py:219\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m   \u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m  \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/models/llama.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     logger,\n\u001b[1;32m     31\u001b[0m     BaseModelOutputWithPast,\n\u001b[1;32m     32\u001b[0m     CausalLMOutputWithPast,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     _prepare_4d_causal_attention_mask_for_sdpa,\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HAS_FLASH_ATTENTION:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/kernels/__init__.py:49\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfast_lora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     get_lora_parameters,\n\u001b[1;32m     39\u001b[0m     get_lora_parameters_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     fast_lora_forward,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     HAS_FLEX_ATTENTION,\n\u001b[1;32m     51\u001b[0m     slow_attention_softcapping,\n\u001b[1;32m     52\u001b[0m     slow_inference_attention_softcapping,\n\u001b[1;32m     53\u001b[0m     create_flex_attention_causal_mask,\n\u001b[1;32m     54\u001b[0m     create_flex_attention_sliding_window_mask,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSLOTH_ZOO_IS_PRESENT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/unsloth/kernels/flex_attention.py:45\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m HAS_FLEX_ATTENTION:\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Logit softcapping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch_compile_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mslow_attention_softcapping\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:1705\u001b[0m, in \u001b[0;36mfn\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_value\u001b[39m(cond, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Throws error containing an optional message if the specified condition\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m    is False.\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03m    Error type: ``ValueError``\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \n\u001b[1;32m   1700\u001b[0m \u001b[38;5;124;03m    C++ equivalent: ``TORCH_CHECK_VALUE``\u001b[39;00m\n\u001b[1;32m   1701\u001b[0m \n\u001b[1;32m   1702\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;124;03m        cond (:class:`bool`): If False, throw error\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;124;03m        message (Callable, optional): Callable that returns either a string or\u001b[39;00m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;124;03m            an object that has a ``__str__()`` method to be used as the error\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;124;03m            message. Default: ``None``\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1709\u001b[0m     _check_with(\u001b[38;5;167;01mValueError\u001b[39;00m, cond, message)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:1723\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_type\u001b[39m(cond, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Throws error containing an optional message if the specified condition\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;124;03m    is False.\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \n\u001b[1;32m   1716\u001b[0m \u001b[38;5;124;03m    Error type: ``TypeError``\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    C++ equivalent: ``TORCH_CHECK_TYPE``\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \n\u001b[1;32m   1720\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m        cond (:class:`bool`): If False, throw error\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \n\u001b[0;32m-> 1723\u001b[0m \u001b[38;5;124;03m        message (Callable, optional): Callable that returns either a string or\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;124;03m            an object that has a ``__str__()`` method to be used as the error\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;124;03m            message. Default: ``None``\u001b[39;00m\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m     _check_with(\u001b[38;5;167;01mTypeError\u001b[39;00m, cond, message)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:594\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[0m\n\u001b[1;32m    589\u001b[0m # Save the function pointer to find the original callable while nesting\n\u001b[1;32m    590\u001b[0m # of decorators.\n\u001b[1;32m    591\u001b[0m _fn._torchdynamo_orig_callable = fn  # type: ignore[attr-defined]\n\u001b[1;32m    593\u001b[0m # when compiling user function instead of nn.Module\n\u001b[0;32m--> 594\u001b[0m # provide public api _fn.get_compiler_config()\n\u001b[1;32m    595\u001b[0m assert not hasattr(_fn, \"get_compiler_config\")\n\u001b[1;32m    596\u001b[0m _fn.get_compiler_config = get_compiler_config  # type: ignore[attr-defined]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:515\u001b[0m, in \u001b[0;36mget_compiler_fn\u001b[0;34m(compiler_fn)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     filename \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcefile(fn)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompiledFn\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     AccuracyError,\n\u001b[1;32m     18\u001b[0m     backend_accuracy_fails,\n\u001b[1;32m     19\u001b[0m     BUCK_CMD_PREFIX,\n\u001b[1;32m     20\u001b[0m     BuckTargetWriter,\n\u001b[1;32m     21\u001b[0m     extra_imports,\n\u001b[1;32m     22\u001b[0m     generate_config_string,\n\u001b[1;32m     23\u001b[0m     helper_for_dump_minify,\n\u001b[1;32m     24\u001b[0m     InputReader,\n\u001b[1;32m     25\u001b[0m     InputWriter,\n\u001b[1;32m     26\u001b[0m     minifier_dir,\n\u001b[1;32m     27\u001b[0m     NNModuleToString,\n\u001b[1;32m     28\u001b[0m     NopInputReader,\n\u001b[1;32m     29\u001b[0m     run_fwd_maybe_bwd,\n\u001b[1;32m     30\u001b[0m     same_two_models,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx_placeholder_targets\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_utils\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rand_strided\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_float_dtype\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreductions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/testing.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebugging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aot_eager\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputGraph\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, eval_frame, optimize_assert, reset\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/backends/debugging.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m min_cut_rematerialization_partition\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _guards\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m functorch_config\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/functorch/compile/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maot_autograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     aot_function,\n\u001b[1;32m      4\u001b[0m     aot_module,\n\u001b[1;32m      5\u001b[0m     aot_module_simplified,\n\u001b[1;32m      6\u001b[0m     compiled_function,\n\u001b[1;32m      7\u001b[0m     compiled_module,\n\u001b[1;32m      8\u001b[0m     get_aot_compilation_context,\n\u001b[1;32m      9\u001b[0m     get_aot_graph_name,\n\u001b[1;32m     10\u001b[0m     get_graph_being_compiled,\n\u001b[1;32m     11\u001b[0m     make_boxed_compiler,\n\u001b[1;32m     12\u001b[0m     make_boxed_func,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     debug_compile,\n\u001b[1;32m     16\u001b[0m     default_decompositions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     ts_compile,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx_minifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minifier\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions_for_rng\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PhiloxStateTracker, rng_decompositions\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_python_dispatcher\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compiled_autograd\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     dynamo_timed,\n\u001b[1;32m     32\u001b[0m     get_chromium_event_logger,\n\u001b[1;32m     33\u001b[0m     preserve_rng_state,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_fake_mode\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/compiled_autograd.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     call_backward,\n\u001b[1;32m     10\u001b[0m     call_hook,\n\u001b[1;32m     11\u001b[0m     FakeCompiledAutogradEngine,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GetItemSource, LocalSource\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m counters, lazy_format_graph_code, set_locals_to_steal\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'call_backward' from 'torch._dynamo.external_utils' (/root/miniconda3/lib/python3.10/site-packages/torch/_dynamo/external_utils.py)"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847f228-7910-43d6-91c6-515b9d9e2ef3",
   "metadata": {},
   "source": [
    "- å°è¯•ç”¨unslothè¿›è¡ŒQwQ-32B-bnb-4bitæ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37319bd-28b9-4c00-9c24-bd7ed75cd922",
   "metadata": {},
   "source": [
    "&emsp;&emsp;é¦–å…ˆè®¾ç½®å…³é”®å‚æ•°ï¼Œå¹¶è¯»å–æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4608195-1e9d-47c6-8c50-e752da195fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c826a-4fe6-4ea2-a67d-9b78b7a9425e",
   "metadata": {},
   "source": [
    "> æ³¨ï¼Œè‹¥æ˜¾å­˜å……è¶³ï¼Œåˆ™å¯ä»¥load_in_4bit = Falseï¼Œè¿›è¡Œå…¨ç²¾åº¦è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825bc87f-731e-4ccc-8b75-2815a0ea1038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H800 PCIe. Num GPUs = 2. Max memory: 79.205 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8d63312951496eb1b181f3095a12f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./QwQ-32B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8ca04-2d64-40a3-9df0-89e9dbaca0fe",
   "metadata": {},
   "source": [
    "> åœ¨4-bitåŠ¨æ€é‡åŒ–ä¸‹ï¼ŒQwQ-32Bæ¨¡å‹å®é™…å ç”¨æ˜¾å­˜çº¦ä¸º22Gï¼š\n",
    "> <center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311154252993.png\" alt=\"image-20250311154252993\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e19d8-365d-4464-badb-1bc1d6ec77dd",
   "metadata": {},
   "source": [
    "æ­¤æ—¶modelå°±æ˜¯è¯»å–è¿›æ¥çš„QwQ-32B 4bitåŠ¨æ€é‡åŒ–æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f687092f-94c7-450a-b251-fceb26e5a716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (1-3): 3 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (4-5): 2 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (6-42): 37 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (43): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (44-59): 16 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (60): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (61): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (62): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (63): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41d797-5505-415d-93f8-2e6adaf4f961",
   "metadata": {},
   "source": [
    "è€Œtokenizeråˆ™æ˜¯åˆ†è¯å™¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bac590-0f5b-4c71-98ea-320c00d69b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='./QwQ-32B-unsloth-bnb-4bit', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7998c2-3505-45e6-98bd-e913609b0530",
   "metadata": {},
   "source": [
    "å°†æ¨¡å‹è°ƒæ•´ä¸ºæ¨ç†æ¨¡å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443cad53-991e-4904-8c03-e06259020939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (1-3): 3 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (4-5): 2 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (6-42): 37 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (43): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (44-59): 16 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (60): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (61): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (62): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (63): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63eb8c-ace2-4287-b924-6d8783ad2c95",
   "metadata": {},
   "source": [
    "#### 2.å¸¦å…¥é—®ç­”æ¨¡æ¿è¿›è¡Œå›ç­”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aee95-a38d-4e3b-9a81-3cf1050a1ecf",
   "metadata": {},
   "source": [
    "- ç»“æ„åŒ–è¾“å…¥æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b427d8-6931-406f-a2ab-38fdb38b613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style_chat = \"\"\"è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9fc959f-0190-4307-aa0b-43d26a81b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d63aa1-48da-4173-87f0-12a334596f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\\n\\n### Question:\\nä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\\n\\n### Response:\\n<think>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prompt_style_chat.format(question, \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9fe11da-3433-4c80-8723-861cce6ee031",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45b4bf8-c61f-4276-bfed-86a28b34fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1be62f68-2c81-4056-b678-948a5bc893f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7459285-9821-4828-a26f-9c9c70b42420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\\n\\n### Question:\\nä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\\n\\n### Response:\\n<think>\\nå—¯ï¼Œç”¨æˆ·è·Ÿæˆ‘è¯´â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œæˆ‘éœ€è¦å›åº”ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥è¡¨è¾¾åŒæ ·çš„é—®å€™ï¼Œç„¶åå¯èƒ½é—®é—®ä»–æœ€è¿‘åœ¨å¿™ä»€ä¹ˆï¼Œæˆ–è€…æœ‰ä»€ä¹ˆæ–°é²œäº‹ã€‚è¦ä¿æŒå‹å¥½å’Œäº²åˆ‡ï¼ŒåŒæ—¶é¼“åŠ±è¿›ä¸€æ­¥çš„å¯¹è¯ã€‚å¯èƒ½è¿˜è¦å›å¿†ä¹‹å‰èŠè¿‡çš„å†…å®¹ï¼Œä½†å¦‚æœæ˜¯çœŸçš„å¾ˆä¹…æ²¡è”ç³»ï¼Œå¯èƒ½è®°ä¸æ¸…ï¼Œæ‰€ä»¥æœ€å¥½ç”¨æ¯”è¾ƒé€šç”¨çš„é—®é¢˜ã€‚æ¯”å¦‚â€œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿâ€æˆ–è€…â€œæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿâ€ã€‚ä¸è¿‡ç”¨æˆ·å¯èƒ½å¸Œæœ›æœ‰æ›´å…·ä½“çš„å›åº”ï¼Œæ‰€ä»¥å¯ä»¥åŠ ä¸Šä¸€äº›å¸¸è§çš„è¿‘å†µè¯é¢˜ï¼Œæ¯”å¦‚å·¥ä½œã€å­¦ä¹ æˆ–è€…å…´è¶£çˆ±å¥½ã€‚å¦å¤–ï¼Œè¦ç¡®ä¿è¯­æ°”è‡ªç„¶ï¼Œä¸æ˜¾å¾—å¤ªç”Ÿç¡¬ã€‚æ¯”å¦‚å¯ä»¥è¯´ï¼šâ€œä½ å¥½ï¼ç¡®å®å¥½ä¹…ä¸è§äº†ï¼Œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿâ€è¿™æ ·æ—¢å›åº”äº†å¯¹æ–¹ï¼Œåˆå¼•å¯¼ä»–ç»§ç»­è°ˆè¯ã€‚æˆ–è€…å¯ä»¥æåˆ°ä¹‹å‰èŠè¿‡çš„äº‹æƒ…ï¼Œä½†å¦‚æœæ²¡æœ‰è®°å¿†çš„è¯ï¼Œå°±ä¿æŒå¼€æ”¾å¼çš„æé—®ã€‚å†æ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰è¯­æ³•é”™è¯¯ï¼Œç¡®ä¿æµç•…ã€‚å—¯ï¼Œè¿™æ ·åº”è¯¥å¯ä»¥äº†ã€‚\\n</think>\\n\\nä½ å¥½ï¼ç¡®å®å¥½ä¹…ä¸è§äº†ï¼Œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿ<|im_end|>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16ee5204-8c35-4e63-9e19-2a33894a4b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å—¯ï¼Œç”¨æˆ·è·Ÿæˆ‘è¯´â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œæˆ‘éœ€è¦å›åº”ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥è¡¨è¾¾åŒæ ·çš„é—®å€™ï¼Œç„¶åå¯èƒ½é—®é—®ä»–æœ€è¿‘åœ¨å¿™ä»€ä¹ˆï¼Œæˆ–è€…æœ‰ä»€ä¹ˆæ–°é²œäº‹ã€‚è¦ä¿æŒå‹å¥½å’Œäº²åˆ‡ï¼ŒåŒæ—¶é¼“åŠ±è¿›ä¸€æ­¥çš„å¯¹è¯ã€‚å¯èƒ½è¿˜è¦å›å¿†ä¹‹å‰èŠè¿‡çš„å†…å®¹ï¼Œä½†å¦‚æœæ˜¯çœŸçš„å¾ˆä¹…æ²¡è”ç³»ï¼Œå¯èƒ½è®°ä¸æ¸…ï¼Œæ‰€ä»¥æœ€å¥½ç”¨æ¯”è¾ƒé€šç”¨çš„é—®é¢˜ã€‚æ¯”å¦‚â€œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿâ€æˆ–è€…â€œæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿâ€ã€‚ä¸è¿‡ç”¨æˆ·å¯èƒ½å¸Œæœ›æœ‰æ›´å…·ä½“çš„å›åº”ï¼Œæ‰€ä»¥å¯ä»¥åŠ ä¸Šä¸€äº›å¸¸è§çš„è¿‘å†µè¯é¢˜ï¼Œæ¯”å¦‚å·¥ä½œã€å­¦ä¹ æˆ–è€…å…´è¶£çˆ±å¥½ã€‚å¦å¤–ï¼Œè¦ç¡®ä¿è¯­æ°”è‡ªç„¶ï¼Œä¸æ˜¾å¾—å¤ªç”Ÿç¡¬ã€‚æ¯”å¦‚å¯ä»¥è¯´ï¼šâ€œä½ å¥½ï¼ç¡®å®å¥½ä¹…ä¸è§äº†ï¼Œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿâ€è¿™æ ·æ—¢å›åº”äº†å¯¹æ–¹ï¼Œåˆå¼•å¯¼ä»–ç»§ç»­è°ˆè¯ã€‚æˆ–è€…å¯ä»¥æåˆ°ä¹‹å‰èŠè¿‡çš„äº‹æƒ…ï¼Œä½†å¦‚æœæ²¡æœ‰è®°å¿†çš„è¯ï¼Œå°±ä¿æŒå¼€æ”¾å¼çš„æé—®ã€‚å†æ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰è¯­æ³•é”™è¯¯ï¼Œç¡®ä¿æµç•…ã€‚å—¯ï¼Œè¿™æ ·åº”è¯¥å¯ä»¥äº†ã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½ï¼ç¡®å®å¥½ä¹…ä¸è§äº†ï¼Œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿ<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc39741-01df-4adb-88fb-885bc4340e2e",
   "metadata": {},
   "source": [
    "- å¤æ‚é—®é¢˜æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c89626c-b70a-42a5-8a91-c21c8b28cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"è¯·è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°ã€‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62ad1369-ccd4-4885-872a-686277148c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53a02c58-9c03-4e22-bb38-d5f8eb13d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "723a82fc-2e5e-48c4-9460-1bf9d1bd2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c862fa3-ffe2-4736-829d-8f201716dddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å—¯ï¼Œç”¨æˆ·è®©æˆ‘è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°ï¼Œè¿™åº”è¯¥æ˜¯ä¸€ä¸ªç»å…¸çš„æ•°å­¦è¯æ˜é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦å›å¿†ä¸€ä¸‹å¦‚ä½•è¯æ˜ä¸€ä¸ªæ•°æ˜¯æ— ç†æ•°ã€‚é€šå¸¸çš„æ–¹æ³•æ˜¯åè¯æ³•ï¼Œå‡è®¾å®ƒæ˜¯æœ‰ç†æ•°ï¼Œç„¶åæ¨å¯¼å‡ºçŸ›ç›¾ã€‚é‚£æ ¹å·2çš„æƒ…å†µåº”è¯¥æ˜¯ç±»ä¼¼çš„ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œå‡è®¾æ ¹å·2æ˜¯æœ‰ç†æ•°ï¼Œé‚£ä¹ˆå®ƒå¯ä»¥è¡¨ç¤ºä¸ºä¸¤ä¸ªäº’è´¨çš„æ•´æ•°aå’Œbçš„æ¯”å€¼ï¼Œä¹Ÿå°±æ˜¯âˆš2 = a/bï¼Œå…¶ä¸­aå’Œbéƒ½æ˜¯æ•´æ•°ï¼Œä¸”äº’è´¨ï¼Œä¹Ÿå°±æ˜¯å®ƒä»¬çš„æœ€å¤§å…¬çº¦æ•°æ˜¯1ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦å¹³æ–¹ä¸¤è¾¹å¾—åˆ°2 = (aÂ²)/(bÂ²)ï¼Œä¹Ÿå°±æ˜¯aÂ² = 2bÂ²ã€‚è¿™è¯´æ˜aÂ²æ˜¯å¶æ•°ï¼Œå› æ­¤aä¹Ÿå¿…é¡»æ˜¯å¶æ•°ï¼Œå› ä¸ºå¥‡æ•°çš„å¹³æ–¹è¿˜æ˜¯å¥‡æ•°ã€‚æ‰€ä»¥ï¼Œå¯ä»¥è®¾a=2kï¼Œå…¶ä¸­kæ˜¯æ•´æ•°ã€‚\n",
      "\n",
      "ä»£å…¥åŸå¼ï¼Œå¾—åˆ°(2k)Â² = 2bÂ²ï¼Œä¹Ÿå°±æ˜¯4kÂ²=2bÂ²ï¼Œç®€åŒ–åæ˜¯2kÂ² = bÂ²ã€‚è¿™æ ·bÂ²ä¹Ÿæ˜¯å¶æ•°ï¼Œæ‰€ä»¥bä¹Ÿå¿…é¡»æ˜¯å¶æ•°ã€‚ä½†ç°åœ¨çš„é—®é¢˜æ¥äº†ï¼Œå› ä¸ºaå’Œbéƒ½æ˜¯å¶æ•°ï¼Œå®ƒä»¬éƒ½å«æœ‰å› å­2ï¼Œè¿™å°±ä¸æœ€åˆå‡è®¾çš„aå’Œbäº’è´¨çŸ›ç›¾äº†ã€‚å› ä¸ºå¦‚æœä¸¤ä¸ªæ•°éƒ½æ˜¯å¶æ•°ï¼Œå®ƒä»¬çš„æœ€å¤§å…¬çº¦æ•°è‡³å°‘æ˜¯2ï¼Œè€Œä¸æ˜¯1ã€‚å› æ­¤ï¼ŒåŸæ¥çš„å‡è®¾ä¸æˆç«‹ï¼Œæ ¹å·2ä¸æ˜¯æœ‰ç†æ•°ï¼Œè€Œæ˜¯æ— ç†æ•°ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œæˆ‘éœ€è¦ç¡®è®¤ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹æœ‰æ²¡æœ‰å“ªé‡Œå‡ºé”™ã€‚æ¯”å¦‚ï¼Œæ˜¯å¦åœ¨ä»£å…¥çš„æ—¶å€™æœ‰æ²¡æœ‰è®¡ç®—é”™è¯¯ï¼Œæˆ–è€…æ˜¯å¦åœ¨æ¨å¯¼è¿‡ç¨‹ä¸­å“ªé‡Œé€»è¾‘ä¸ä¸¥å¯†ã€‚æ¯”å¦‚ï¼Œæ˜¯å¦aæ˜¯å¶æ•°çš„æ¨å¯¼æ­£ç¡®ï¼Ÿæ˜¯çš„ï¼Œå› ä¸ºå¦‚æœaÂ²æ˜¯å¶æ•°ï¼Œé‚£ä¹ˆaå¿…é¡»æ˜¯å¶æ•°ï¼Œå› ä¸ºå¥‡æ•°çš„å¹³æ–¹ä¸ä¼šæ˜¯å¶æ•°ã€‚åŒæ ·ï¼ŒbÂ²æ˜¯å¶æ•°çš„è¯ï¼Œbä¹Ÿå¿…é¡»æ˜¯å¶æ•°ã€‚è¿™æ ·ç¡®å®å¯¼è‡´çŸ›ç›¾ï¼Œæ‰€ä»¥ç»“è®ºæ˜¯å¯¹çš„ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œå¯èƒ½ç”¨æˆ·éœ€è¦æ›´è¯¦ç»†çš„æ­¥éª¤ï¼Œæˆ–è€…æœ‰æ²¡æœ‰å…¶ä»–æ–¹æ³•ï¼Ÿæ¯”å¦‚ç”¨è´¨æ•°åˆ†è§£æˆ–è€…å…¶ä»–æ–¹æ³•ã€‚ä¸è¿‡åè¯æ³•åº”è¯¥æ˜¯æœ€ç›´æ¥çš„ã€‚å¯èƒ½ç”¨æˆ·æ˜¯å­¦ç”Ÿï¼Œåˆšå¼€å§‹å­¦æ•°è®ºï¼Œæ‰€ä»¥éœ€è¦è¯¦ç»†è§£é‡Šæ¯ä¸€æ­¥ï¼Œç¡®ä¿ä»–ä»¬ç†è§£äº’è´¨çš„æ¦‚å¿µä»¥åŠä¸ºä»€ä¹ˆçŸ›ç›¾å‡ºç°ã€‚æˆ–è€…ç”¨æˆ·å¯èƒ½æƒ³ç¡®è®¤è¿™ä¸ªè¯æ˜çš„æ­£ç¡®æ€§ï¼Œæ‰€ä»¥éœ€è¦æ¸…æ™°çš„é€»è¾‘æ­¥éª¤ã€‚\n",
      "\n",
      "æœ‰æ²¡æœ‰å¯èƒ½ç”¨æˆ·æœ‰æ›´æ·±å±‚æ¬¡çš„éœ€æ±‚ï¼Ÿæ¯”å¦‚ä»–ä»¬å¯èƒ½åœ¨å‡†å¤‡è€ƒè¯•ï¼Œæˆ–è€…éœ€è¦è¿™ä¸ªè¯æ˜ä½œä¸ºå…¶ä»–é—®é¢˜çš„åŸºç¡€ã€‚ä¸ç®¡æ€æ ·ï¼Œæä¾›ä¸€ä¸ªæ¸…æ™°çš„æ­¥éª¤è§£é‡Šåº”è¯¥æ˜¯è¶³å¤Ÿçš„ã€‚å¦å¤–ï¼Œå¯èƒ½éœ€è¦æé†’ä»–ä»¬ï¼Œè¿™ä¸ªè¯æ˜æ˜¯ç»å…¸çš„ï¼Œé€šå¸¸è¢«ç§°ä¸ºæ¯•è¾¾å“¥æ‹‰æ–¯å­¦æ´¾çš„å‘ç°ï¼Œå¯¼è‡´äº†æ— ç†æ•°çš„æ¦‚å¿µã€‚ä½†å¯èƒ½ä¸éœ€è¦æ¶‰åŠå†å²èƒŒæ™¯ï¼Œé™¤éç”¨æˆ·é—®èµ·ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼ŒæŒ‰ç…§åè¯æ³•çš„æ­¥éª¤ï¼Œä¸€æ­¥æ­¥å±•ç¤ºçŸ›ç›¾ï¼Œåº”è¯¥å°±èƒ½è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°äº†ã€‚éœ€è¦ç¡®ä¿æ¯ä¸€æ­¥éƒ½æ­£ç¡®ï¼Œå¹¶ä¸”é€»è¾‘è¿è´¯ï¼Œæ²¡æœ‰æ¼æ´ã€‚\n",
      "</think>\n",
      "\n",
      "è¦è¯æ˜âˆš2æ˜¯æ— ç†æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åè¯æ³•ï¼š\n",
      "\n",
      "**è¯æ˜è¿‡ç¨‹ï¼š**\n",
      "\n",
      "1. **å‡è®¾ç›¸åå‘½é¢˜æˆç«‹**  \n",
      "   å‡è®¾âˆš2æ˜¯æœ‰ç†æ•°ï¼Œé‚£ä¹ˆå®ƒå¯ä»¥è¡¨ç¤ºä¸ºä¸¤ä¸ªäº’è´¨çš„æ•´æ•°aå’Œbçš„æ¯”å€¼ï¼Œå³ï¼š  \n",
      "   \\[\n",
      "   \\sqrt{2} = \\frac{a}{b} \\quad (a,b \\in \\mathbb{Z},\\ b \\neq 0,\\ \\text{ä¸”} \\gcd(a,b)=1)\n",
      "   \\]\n",
      "\n",
      "2. **å¹³æ–¹ä¸¤è¾¹**  \n",
      "   ä¸¤è¾¹å¹³æ–¹å¾—ï¼š  \n",
      "   \\[\n",
      "   2 = \\frac{a^2}{b^2} \\quad \\Rightarrow \\quad a^2 = 2b^2\n",
      "   \\]\n",
      "\n",
      "3. **æ¨å¯¼çŸ›ç›¾**  \n",
      "   - **aæ˜¯å¶æ•°**  \n",
      "     ç”±\\( a^2 = 2b^2 \\)å¯çŸ¥ï¼Œ\\( a^2 \\)æ˜¯å¶æ•°ã€‚å› ä¸ºå¥‡æ•°çš„å¹³æ–¹ä»ä¸ºå¥‡æ•°ï¼Œå› æ­¤aå¿…é¡»æ˜¯å¶æ•°ã€‚è®¾\\( a = 2k \\)ï¼ˆkä¸ºæ•´æ•°ï¼‰ã€‚  \n",
      "   - **ä»£å…¥å¹¶ç®€åŒ–**  \n",
      "     å°†\\( a = 2k \\)ä»£å…¥ä¸Šå¼ï¼š  \n",
      "     \\[\n",
      "     (2k)^2 = 2b^2 \\quad \\Rightarrow \\quad 4k^2 = 2b^2 \\quad \\Rightarrow \\quad b^2 = 2k^2\n",
      "     \\]  \n",
      "     åŒç†ï¼Œ\\( b^2 \\)æ˜¯å¶æ•°ï¼Œæ•…bä¹Ÿå¿…ä¸ºå¶æ•°ã€‚\n",
      "\n",
      "4. **çŸ›ç›¾ç»“è®º**  \n",
      "   æ­¤æ—¶aå’Œbå‡ä¸ºå¶æ•°ï¼Œæ„å‘³ç€å®ƒä»¬çš„æœ€å¤§å…¬çº¦æ•°è‡³å°‘ä¸º2ï¼Œè¿™ä¸åˆå§‹å‡è®¾\\(\\gcd(a,b)=1\\)çŸ›ç›¾ã€‚å› æ­¤ï¼ŒåŸå‡è®¾â€œâˆš2æ˜¯æœ‰ç†æ•°â€ä¸æˆç«‹ã€‚\n",
      "\n",
      "**ç»“è®º**  \n",
      "âˆš2ä¸èƒ½è¡¨ç¤ºä¸ºä¸¤ä¸ªæ•´æ•°çš„æ¯”å€¼ï¼Œå› æ­¤å®ƒæ˜¯æ— ç†æ•°ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "è¿™ä¸ªè¯æ˜çš„æ ¸å¿ƒåœ¨äºé€šè¿‡åè¯æ³•æ­ç¤ºçŸ›ç›¾ï¼Œå±•ç¤ºäº†æœ‰ç†æ•°å‡è®¾çš„ä¸è‡ªæ´½æ€§ã€‚å¸Œæœ›è¿™ä¸ªè¿‡ç¨‹æ¸…æ™°æ˜“æ‡‚ï¼<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258079b-67cd-43ef-8e5e-c8d20c6cc62f",
   "metadata": {},
   "source": [
    "#### 3.åŸå§‹æ¨¡å‹çš„åŒ»ç–—é—®é¢˜é—®ç­”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7dda0e-4a98-4ded-9be8-cc60ffb9f356",
   "metadata": {},
   "source": [
    "- é‡æ–°è®¾ç½®é—®ç­”æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dadf6c0-53c1-4d07-b0ec-57b723198f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b915e-914e-41e4-b6be-d81ab77a7c8f",
   "metadata": {},
   "source": [
    "ç¿»è¯‘å¦‚ä¸‹ï¼š\n",
    "\n",
    "```python\n",
    "prompt_style = \"\"\"ä»¥ä¸‹æ˜¯ä¸€ä¸ªä»»åŠ¡è¯´æ˜ï¼Œé…æœ‰æä¾›æ›´å¤šèƒŒæ™¯ä¿¡æ¯çš„è¾“å…¥ã€‚\n",
    "è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆè¯¥ä»»åŠ¡ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶æŒ‰æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œç¡®ä¿å›ç­”é€»è¾‘æ¸…æ™°ä¸”å‡†ç¡®ã€‚\n",
    "\n",
    "### Instruction:\n",
    "æ‚¨æ˜¯ä¸€ä½å…·æœ‰é«˜çº§ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è§„åˆ’çŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea90929-b517-414c-8399-0a6f783b7c26",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æ¥ä¸‹æ¥æˆ‘ä»¬æŠ½å–éƒ¨åˆ†medical-o1-reasoning-SFTæ•°æ®é›†ä¸­é—®é¢˜è¿›è¡Œæé—®ï¼Œå¹¶æŸ¥çœ‹åˆå§‹çŠ¶æ€ä¸‹æ¨¡å‹å›ç­”ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fee56999-513f-4a2b-9be5-677821c994f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e98f9e-6c5b-42f3-8e78-07951cdc069c",
   "metadata": {},
   "source": [
    "ç¿»è¯‘ï¼šä¸€ä½61å²çš„å¥³æ€§ï¼Œæœ‰é•¿æœŸåœ¨å’³å—½æˆ–æ‰“å–·åšç­‰æ´»åŠ¨ä¸­å‘ç”Ÿä¸è‡ªä¸»å°¿æ¶²æµå¤±çš„ç—…å²ï¼Œä½†å¤œé—´æ²¡æœ‰æ¼å°¿ã€‚å¥¹æ¥å—äº†å¦‡ç§‘æ£€æŸ¥å’ŒQ-tipæµ‹è¯•ã€‚æ ¹æ®è¿™äº›æ£€æŸ¥ç»“æœï¼Œè†€èƒ±æµ‹é‡ï¼ˆcystometryï¼‰æœ€å¯èƒ½ä¼šæ˜¾ç¤ºå¥¹çš„æ®‹ä½™å°¿é‡å’Œé€¼å°¿è‚Œæ”¶ç¼©æƒ…å†µå¦‚ä½•ï¼Ÿ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96bffeb4-8252-4e4e-8c3e-9a3375ab940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623b554-cae5-4a9f-b033-9c356ae031ad",
   "metadata": {},
   "source": [
    "ç¿»è¯‘ï¼šé¢å¯¹ä¸€ä½çªå‘èƒ¸ç—›å¹¶æ”¾å°„è‡³é¢ˆéƒ¨å’Œå·¦è‡‚çš„æ‚£è€…ï¼Œå…¶æ—¢å¾€ç—…å²åŒ…æ‹¬é«˜èƒ†å›ºé†‡è¡€ç—‡å’Œå† çŠ¶åŠ¨è„‰ç–¾ç—…ï¼ŒåŒæ—¶ä¼´æœ‰å‡é«˜çš„è‚Œé’™è›‹ç™½Iæ°´å¹³å’Œå¿ƒåŠ¨è¿‡é€Ÿï¼Œæ ¹æ®è¿™äº›ä¸´åºŠè¡¨ç°ï¼Œæœ€å¯èƒ½å—ç´¯çš„å† çŠ¶åŠ¨è„‰æ˜¯å“ªä¸€æ¡ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e929f7-614c-4f86-9cd0-b6417c1adbd1",
   "metadata": {},
   "source": [
    "- é—®ç­”æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c2290e9-1e9e-4fef-bde8-cc84591437bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs1 = model.generate(\n",
    "    input_ids=inputs1.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response1 = tokenizer.batch_decode(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02e6c870-23e6-4f6f-a990-720dfc36887b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, so I need to figure out what cystometry would show for this 61-year-old woman with urinary incontinence during activities like coughing or sneezing. Let me start by recalling the basics.\n",
      "\n",
      "First, her symptoms: she has involuntary urine loss during physical activities. That sounds like stress urinary incontinence (SUI). SUI is usually due to weakened pelvic floor muscles or urethral sphincter, leading to leakage when there's increased abdominal pressure, like coughing, sneezing, or exercising. The fact that she doesn't leak at night suggests it's not an overactive bladder (OAB) or something nocturnal like nocturia from another cause.\n",
      "\n",
      "Now, the Q-tip test. I remember that the Q-tip test is used to assess urethral mobility. During the test, the patient is asked to strain or cough while the angle of the urethra's movement is measured. If the angle increases beyond a certain point (like more than 30 degrees), it indicates urethral hypermobility, which is a common cause of SUI. So if her Q-tip test was positive, that supports the diagnosis of SUI due to anatomical issues.\n",
      "\n",
      "Cystometry is a test that measures bladder pressure and volume during filling and voiding. The key things cystometry looks at are bladder compliance, presence of detrusor contractions, and post-void residual urine.\n",
      "\n",
      "The question asks about residual volume and detrusor contractions. Let's break it down:\n",
      "\n",
      "Residual volume: In SUI, since the issue is about urine leaking during increased pressure, the bladder's ability to store urine might not be the problem. So during cystometry, when they measure the residual urine after voiding, it should be normal (or low). High residual would suggest underactive detrusor or obstruction, which isn't typical for SUI.\n",
      "\n",
      "Detrusor contractions: Overactive bladder is characterized by involuntary detrusor contractions (detrusor overactivity) leading to urgency and urge incontinence. But in SUI, the problem isn't the bladder muscle's activity. Instead, the detrusor should be stable without involuntary contractions during the filling phase. So cystometry would likely show no detrusor contractions during filling, which differentiates it from urge incontinence.\n",
      "\n",
      "Wait, but I should make sure there's no mixed incontinence. The question says \"no leakage at night,\" which might reduce the chance of OAB, but maybe there's a little? But the key here is that the primary symptom is stress-related. The cystometry would help confirm that there's no detrusor overactivity. So the main findings would be normal residual volume and absence of detrusor contractions during the test.\n",
      "\n",
      "Putting it all together: cystometry would show normal residual volume (since she can empty well) and no detrusor contractions (since it's not urge incontinence). So the answer should state that cystometry would reveal a normal or low residual volume and absent detrusor contractions during filling.\n",
      "</think>\n",
      "\n",
      "The patient's symptoms of involuntary urine loss during activities like coughing or sneezing, combined with a negative nighttime leakage history, strongly suggest **stress urinary incontinence (SUI)**. The Q-tip test likely demonstrates urethral hypermobility, a common cause of SUI. \n",
      "\n",
      "### Cystometry Findings:\n",
      "1. **Residual Volume**:  \n",
      "   - **Normal or low residual volume** after voiding.  \n",
      "   - Rationale: SUI is caused by anatomical weakness (e.g., pelvic floor or urethral sphincter dysfunction), not impaired bladder emptying. A normal residual volume confirms the bladder is storing and emptying urine appropriately, ruling out underactive detrusor or obstruction.\n",
      "\n",
      "2. **Detrusor Contractions**:  \n",
      "   - **No involuntary detrusor contractions during bladder filling**.  \n",
      "   - Rationale: Detrusor overactivity (involuntary contractions) is a hallmark of overactive bladder (OAB)/urge incontinence, which is unrelated to SUI. The absence of detrusor contractions on cystometry differentiates SUI from mixed incontinence and confirms the primary issue is anatomical stress-related leakage rather than bladder instability.\n",
      "\n",
      "### Conclusion:  \n",
      "Cystometry would reveal **normal residual volume** and **absence of detrusor contractions** during the filling phase, supporting a diagnosis of **pure stress urinary incontinence**. This contrasts with urge incontinence or mixed incontinence, which would show detrusor overactivity on testing.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response1[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9addf3d-8181-4fbb-808f-27f1dd65f274",
   "metadata": {},
   "source": [
    "ç¿»è¯‘å¦‚ä¸‹ï¼š\n",
    "\n",
    "\\<think>  \n",
    "å¥½çš„ï¼Œæˆ‘æ­£åœ¨å°è¯•åˆ†æè¿™ä¸ªåŒ»å­¦é—®é¢˜ã€‚æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥åˆ†è§£ã€‚æ‚£è€…æ˜¯ä¸€ä½61å²çš„å¥³æ€§ï¼Œæœ‰åœ¨å’³å—½æˆ–æ‰“å–·åšç­‰æ´»åŠ¨ä¸­å‘ç”Ÿä¸è‡ªä¸»å°¿æ¶²æµå¤±çš„ç—…å²ï¼Œä½†å¥¹å¤œé—´æ²¡æœ‰æ¼å°¿ã€‚å¥¹æ­£åœ¨æ¥å—å¦‡ç§‘æ£€æŸ¥å’ŒQ-tipæµ‹è¯•ã€‚é—®é¢˜æ˜¯å…³äºè†€èƒ±æµ‹é‡ï¼ˆcystometryï¼‰ä¼šæ˜¾ç¤ºå¥¹çš„æ®‹ä½™å°¿é‡å’Œé€¼å°¿è‚Œæ”¶ç¼©æƒ…å†µã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘çŸ¥é“åœ¨åƒæ‰“å–·åšæˆ–å’³å—½ç­‰æ´»åŠ¨ä¸­å‘ç”Ÿä¸è‡ªä¸»å°¿æ¶²æµå¤±é€šå¸¸ä¸å‹åŠ›æ€§å°¿å¤±ç¦æœ‰å…³ã€‚å‹åŠ›æ€§å°¿å¤±ç¦é€šå¸¸å‘ç”Ÿåœ¨å°¿é“è‚Œè‚‰ä¸è¶³ä»¥åœ¨å‹åŠ›å¢å¤§çš„æƒ…å†µä¸‹ï¼ˆæ¯”å¦‚å’³å—½æ—¶ï¼‰é˜²æ­¢è†€èƒ±æ¼å°¿æ—¶ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥æ˜¯Q-tipæµ‹è¯•ã€‚æ ¹æ®æˆ‘è®°å¾—çš„ï¼ŒQ-tipæ˜¯ä¸€ç§ç”¨äºæµ‹é‡å°¿é“å‹åŠ›æ›²çº¿çš„å°¿é“å¯¼ç®¡ã€‚å®ƒé€šå¸¸ç”¨äºè¯„ä¼°å°¿é“åŠŸèƒ½ã€‚Q-tipæµ‹è¯•é˜³æ€§ç»“æœï¼Œå³åœ¨Valsalvaæ“ä½œè¿‡ç¨‹ä¸­å°¿é“å‹åŠ›ä½äºè†€èƒ±å†…å‹ï¼Œä¸å†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·ç›¸å…³ï¼Œè¿™æ˜¯ä¸€ç§å‹åŠ›æ€§å°¿å¤±ç¦ç±»å‹ã€‚\n",
    "\n",
    "ç”±äºæ‚£è€…æœ‰åœ¨æ´»åŠ¨ä¸­å‡ºç°ä¸è‡ªä¸»æ¼å°¿çš„ç—…å²ï¼Œä½†å¤œé—´æ²¡æœ‰æ¼å°¿ï¼Œæ›´å¯èƒ½æ˜¯å‹åŠ›æ€§å°¿å¤±ç¦ï¼Œè€Œä¸æ˜¯åƒæ€¥è¿«æ€§å°¿å¤±ç¦é‚£æ ·çš„æƒ…å†µï¼Œæ€¥è¿«æ€§å°¿å¤±ç¦é€šå¸¸ä¼´æœ‰å¤œé—´æ¼å°¿ã€‚å› æ­¤ï¼Œå¦‚æœQ-tipæµ‹è¯•é˜³æ€§ï¼Œæç¤ºå†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·ã€‚\n",
    "\n",
    "ç°åœ¨ï¼Œè°ˆåˆ°è†€èƒ±æµ‹é‡ã€‚è†€èƒ±æµ‹é‡æ˜¯ä¸€ç§æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹é‡è†€èƒ±åœ¨å……ç›ˆè¿‡ç¨‹ä¸­çš„ååº”ä»¥åŠé€¼å°¿è‚Œçš„æ”¶ç¼©æƒ…å†µã€‚å®ƒå¯ä»¥æ˜¾ç¤ºæ˜¯å¦å­˜åœ¨è†€èƒ±è¿‡åº¦æ´»åŠ¨ç—‡ï¼ˆOABï¼‰ï¼Œå³å¼•èµ·æ€¥è¿«æ„Ÿå’Œé¢‘å°¿çš„æƒ…å†µï¼Œæˆ–æ˜¯å¦å­˜åœ¨é€¼å°¿è‚Œä½æ´»åŠ¨æ€§ï¼Œå¯¼è‡´å°¿æ½´ç•™ã€‚\n",
    "\n",
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…çš„ä¸»è¦é—®é¢˜æ˜¯å‹åŠ›æ€§å°¿å¤±ç¦ï¼Œè¿™æ›´ä¸æ— æ³•åœ¨å‹åŠ›å¢å¤§æ—¶ä¿æŒå°¿æ¶²æœ‰å…³ã€‚è†€èƒ±æµ‹é‡ä¼šæŸ¥çœ‹é€¼å°¿è‚Œçš„æ”¶ç¼©æƒ…å†µã€‚å¦‚æœé€¼å°¿è‚Œä½æ´»åŠ¨æ€§ï¼Œå®ƒå°†ä¸èƒ½å¼ºæœ‰åŠ›åœ°æ”¶ç¼©ä»¥æ’ç©ºè†€èƒ±ï¼Œå¯¼è‡´æ®‹ä½™å°¿é‡ã€‚ä½†å¦‚æœé€¼å°¿è‚Œè¿‡åº¦æ´»è·ƒï¼Œå¯èƒ½ä¼šæ”¶ç¼©è¿‡åº¦ï¼Œå¯¼è‡´æ€¥è¿«æ„Ÿã€‚\n",
    "\n",
    "é‰´äºæ‚£è€…æœ‰å‹åŠ›æ€§å°¿å¤±ç¦çš„ç—…å²å’ŒQ-tipæµ‹è¯•é˜³æ€§ï¼Œæç¤ºå†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·ï¼Œæˆ‘è®¤ä¸ºè†€èƒ±æµ‹é‡ä¼šæ˜¾ç¤ºé€¼å°¿è‚Œçš„æ”¶ç¼©æ˜¯æ­£å¸¸çš„ã€‚é—®é¢˜ä¸åœ¨äºé€¼å°¿è‚Œæ”¶ç¼©çš„èƒ½åŠ›ï¼Œè€Œæ˜¯æ— æ³•å¯†å°å°¿é“ä»¥ä¿æŒå‹åŠ›ã€‚å› æ­¤ï¼Œæ®‹ä½™å°¿é‡å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œé™¤éæœ‰æ˜æ˜¾çš„å°¿æ½´ç•™ï¼Œä½†å…³é”®å‘ç°æ˜¯é€¼å°¿è‚Œçš„æ”¶ç¼©æ˜¯æ­£å¸¸çš„ï¼Œè€Œä¸æ˜¯è¿‡åº¦æ´»è·ƒã€‚\n",
    "\n",
    "ç­‰ç­‰ï¼Œä½†ä¼šä¸ä¼šæœ‰æ®‹ä½™å°¿é‡ï¼Ÿå¦‚æœæ‚£è€…æ’å°¿åè†€èƒ±ä¸­æ®‹ç•™ä¸€äº›å°¿æ¶²ï¼Œé‚£å°±æ˜¯æ®‹ä½™å°¿é‡ã€‚ä½†å¦‚æœæ²¡æœ‰å°¿æ½´ç•™çš„ç—‡çŠ¶ï¼Œæ¯”å¦‚è†€èƒ±é¥±èƒ€æˆ–æ’å°¿å›°éš¾ï¼Œé‚£ä¹ˆè¿™ç§æƒ…å†µçš„å¯èƒ½æ€§è¾ƒå°ã€‚ä¸»è¦é—®é¢˜æ˜¯åœ¨æ´»åŠ¨ä¸­å‘ç”Ÿçš„å°¿å¤±ç¦ï¼Œå› æ­¤é€¼å°¿è‚Œæ”¶ç¼©æ˜¯æ­£å¸¸çš„ï¼Œæ®‹ä½™å°¿é‡åœ¨æ­£å¸¸èŒƒå›´å†…ï¼Œé™¤éæœ‰å…¶ä»–æƒ…å†µã€‚\n",
    "\n",
    "æ‰€ä»¥ï¼Œç»¼åˆæ¥çœ‹ï¼Œè†€èƒ±æµ‹é‡å¯èƒ½ä¼šæ˜¾ç¤ºé€¼å°¿è‚Œçš„æ”¶ç¼©æ­£å¸¸ï¼Œæ®‹ä½™å°¿é‡æ­£å¸¸ã€‚é—®é¢˜æ›´å¤šæ˜¯åœ¨æ‹¬çº¦è‚Œæ–¹é¢ï¼Œè€Œä¸æ˜¯é€¼å°¿è‚Œã€‚  \n",
    "\\</think>\n",
    "\n",
    "æ ¹æ®å¯¹æ‚£è€…ç—…å²å’ŒQ-tipæµ‹è¯•ç»“æœçš„åˆ†æï¼Œè†€èƒ±æµ‹é‡æœ€å¯èƒ½æ˜¾ç¤ºé€¼å°¿è‚Œçš„æ”¶ç¼©æ­£å¸¸ï¼Œæ®‹ä½™å°¿é‡æ­£å¸¸ã€‚ä¸»è¦é—®é¢˜ä¼¼ä¹æ˜¯ç”±äºå†…æºæ€§æ‹¬çº¦è‚Œç¼ºé™·å¼•èµ·çš„å‹åŠ›æ€§å°¿å¤±ç¦ï¼Œå¦‚Q-tipæµ‹è¯•é˜³æ€§æ‰€ç¤ºã€‚è¿™ç§æƒ…å†µé€šå¸¸å½±å“å°¿é“æ‹¬çº¦è‚Œåœ¨å‹åŠ›å¢å¤§æ—¶é˜²æ­¢æ¼å°¿çš„èƒ½åŠ›ï¼Œè€Œä¸æ˜¯é€¼å°¿è‚Œçš„æ”¶ç¼©èƒ½åŠ›ã€‚å› æ­¤ï¼Œé€¼å°¿è‚Œçš„æ”¶ç¼©å¹¶æœªè¿‡åº¦æ´»è·ƒï¼Œæ®‹ä½™å°¿é‡åœ¨æ­£å¸¸èŒƒå›´å†…ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf7ec2-fcd2-4b19-91c0-0eae77f42ee4",
   "metadata": {},
   "source": [
    "æ ‡å‡†ç­”æ¡ˆï¼š\n",
    "\n",
    "åœ¨è¿™ç§å‹åŠ›æ€§å°¿å¤±ç¦çš„æƒ…å†µä¸‹ï¼Œè†€èƒ±æµ‹å‹æ£€æŸ¥ï¼ˆcystometryï¼‰æœ€å¯èƒ½æ˜¾ç¤º**æ­£å¸¸çš„æ’å°¿åæ®‹ä½™å°¿é‡**ï¼Œå› ä¸ºå‹åŠ›æ€§å°¿å¤±ç¦é€šå¸¸ä¸ä¼šå½±å“è†€èƒ±æ’ç©ºåŠŸèƒ½ã€‚æ­¤å¤–ï¼Œç”±äºå‹åŠ›æ€§å°¿å¤±ç¦ä¸»è¦ä¸**èº«ä½“ç”¨åŠ›**æœ‰å…³ï¼Œè€Œä¸æ˜¯è†€èƒ±è¿‡åº¦æ´»åŠ¨ç—‡ï¼ˆOABï¼‰ï¼Œå› æ­¤åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­**ä¸å¤ªå¯èƒ½è§‚å¯Ÿåˆ°é€¼å°¿è‚Œçš„éè‡ªä¸»æ”¶ç¼©**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2c20cbb-56a9-471a-94a5-b1f605bc3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs2 = model.generate(\n",
    "    input_ids=inputs2.input_ids,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response2 = tokenizer.batch_decode(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "786cca4c-988b-4369-b9e5-56688f1ff643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "\n",
      "Okay, let's tackle this question. The patient has sudden chest pain radiating to the neck and left arm. They have a history of hypercholesterolemia and coronary artery disease. Elevated troponin I suggests myocardial damage, so probably a heart attack. Tachycardia could be a response to pain or the heart issue itself.\n",
      "\n",
      "First, I need to recall the coronary arteries and their territories. The left main coronary artery branches into the left anterior descending (LAD) and the left circumflex (LCX). The right coronary artery (RCA) is the other main one. \n",
      "\n",
      "Chest pain location can sometimes correlate with the artery affected. But radiation to the neck and left arm... Wait, the left arm radiation is classic for cardiac issues. The LAD supplies the anterior wall. The LCX goes around the left side, and the RCA supplies the inferior wall. \n",
      "\n",
      "Wait, the left circumflex artery (LCX) might supply the lateral wall, and sometimes the left main artery if it's a proximal block. But the RCA often supplies the inferior wall, which might radiate to the jaw or upper back, maybe left shoulder. But the left arm radiation is more commonly associated with the left anterior descending artery? Or maybe the left circumflex?\n",
      "\n",
      "Alternatively, the left anterior descending artery supplies the anterior and apex, so maybe radiation to left arm? Wait, the left circumflex might supply the lateral wall and the left arm, but I'm a bit confused here.\n",
      "\n",
      "Wait, let me think again. The left arm pain is often from the median or ulnar nerves, but in cardiac cases, it's due to referred pain. The cardiac pain fibers project to the spinal cord levels of C1-C4 for the heart, so radiation can go to the neck, jaw, left arm. \n",
      "\n",
      "The distribution of the coronary arteries:\n",
      "\n",
      "- LAD: supplies anterior wall, apex, septum. Damage here (like an occlusion) would cause pain radiating to the jaw and left arm, maybe.\n",
      "\n",
      "- Circumflex (LCX): lateral wall. If the dominant artery is LCX, but often the RCA is the dominant one in many people. Wait, the dominance of the coronary arteries matters here. If the RCA is the dominant, then LCX might not be as big. But regardless, the left arm pain... \n",
      "\n",
      "Wait, the left circumflex supplies the lateral left side of the heart. The RCA supplies the inferior wall. Inferior wall MI might radiate to the upper abdomen or back, maybe the right arm? Or maybe the left. Hmm.\n",
      "\n",
      "Alternatively, the left anterior descending artery is more likely to cause pain radiating to the left arm because it's the anterior part. \n",
      "\n",
      "Wait, maybe the left circumflex and left main artery are less common. The RCA is more about inferior. \n",
      "\n",
      "Alternatively, if the left main coronary artery is blocked, that's a big problem, but the symptoms might be more severe. But given the patient has a history of CAD, maybe a specific artery is more prone.\n",
      "\n",
      "Wait, the patient has elevated troponin, so it's a myocardial infarction. The location of the infarction would indicate the artery. \n",
      "\n",
      "Chest pain radiating to the neck and left arm is classic for an anterior or lateral wall MI, which would be from LAD or LCX. \n",
      "\n",
      "But the left arm radiation is more commonly associated with left-sided heart issues. If the RCA is blocked, the inferior wall is affected, which might radiate to the jaw (like inferior to jaw?), or left arm? Wait, some sources say that inferior wall MI can radiate to the back and left arm. \n",
      "\n",
      "Hmm, maybe I need to think about the dermatomal distribution of the cardiac pain. The heart's afferent nerves come from the spinal cord levels T1-T4. The pain can refer to the area innervated by those same spinal nerves. \n",
      "\n",
      "The Cervical nerves might not be directly involved, but T1-T4. So the left arm pain would be from T2-T4, which includes the left arm. \n",
      "\n",
      "But which coronary artery supplies the area that would cause that? \n",
      "\n",
      "The LAD supplies the anterior wall. A block in the LAD could lead to anterior MI, which might present with central chest pain radiating to the left arm and jaw. \n",
      "\n",
      "The LCX supplies the lateral wall. A lateral MI might present with pain radiating to the left arm, but maybe more to the inner left arm. \n",
      "\n",
      "The RCA supplies the inferior wall. Inferior MI might radiate to the back, left shoulder, or arm. \n",
      "\n",
      "Wait, some sources say that the left arm radiation can occur in both anterior and inferior MIs. \n",
      "\n",
      "Alternatively, the left anterior descending artery is the most common site for coronary artery disease, especially in someone with hypercholesterolemia and CAD history. \n",
      "\n",
      "Given that the patient has a history of CAD and hypercholesterolemia, the most likely artery involved would be the left anterior descending (LAD) artery, leading to an anterior wall MI, which can present with the described symptoms. \n",
      "\n",
      "Alternatively, if the RCA is involved, but the left arm radiation might still occur. But the classic presentation for LAD occlusion includes left arm pain. \n",
      "\n",
      "Wait, another angle: the left circumflex artery supplies the left lateral wall. If the circumflex is occluded, the pain might radiate to the left arm. But the LAD is more central. \n",
      "\n",
      "Hmm, but the question is asking for the most likely. Given the radiation to the neck and left arm, and considering the most common coronary artery involved in such cases, it's probably the LAD. \n",
      "\n",
      "Alternatively, the left main coronary artery occlusion would cause widespread symptoms, but that's a more catastrophic event. \n",
      "\n",
      "Wait, maybe the left circumflex? Or perhaps the\n"
     ]
    }
   ],
   "source": [
    "print(response2[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f13f64-11bc-47bb-8994-e1ccea729bb9",
   "metadata": {},
   "source": [
    "ç¿»è¯‘å¦‚ä¸‹ï¼š\n",
    "\n",
    "\\<think>  \n",
    "å¥½çš„ï¼Œæˆ‘æœ‰ä¸€ä½æ‚£è€…ï¼Œçªç„¶å‡ºç°èƒ¸ç—›ï¼Œå¹¶æ”¾å°„åˆ°é¢ˆéƒ¨å’Œå·¦è‡‚ã€‚è¿™è®©æˆ‘æƒ³åˆ°äº†å¿ƒè„ç—…å‘ä½œï¼Œå› ä¸ºè¿™äº›ç—‡çŠ¶å¾ˆç»å…¸â€”â€”å¿ƒç»ç—›æˆ–å¿ƒè‚Œæ¢—æ­»ã€‚å·¦è‡‚ç—›ã€é¢ˆéƒ¨ç—›ï¼Œæœ‰æ—¶è¿˜ä¼šä¼´éšä¸‹é¢Œæˆ–èƒŒéƒ¨çš„ç–¼ç—›ï¼Œè¿™äº›éƒ½å¯èƒ½ä¸å† çŠ¶åŠ¨è„‰é—®é¢˜ç›¸å…³ã€‚\n",
    "\n",
    "ä»ç—…å²æ¥çœ‹ï¼Œæ‚£è€…æœ‰**é«˜èƒ†å›ºé†‡è¡€ç—‡**ï¼ˆå³é«˜èƒ†å›ºé†‡ï¼‰å’Œ**å† çŠ¶åŠ¨è„‰ç–¾ç—…**ï¼Œè¿™ä¸¤ä¸ªå› ç´ éƒ½æ˜¯**åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–**çš„é£é™©å› ç´ ï¼Œå¯èƒ½å¯¼è‡´å† çŠ¶åŠ¨è„‰å‘ç”Ÿå µå¡ã€‚**è‚Œé’™è›‹ç™½Iå‡é«˜**æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„çº¿ç´¢ï¼Œå› ä¸ºè‚Œé’™è›‹ç™½æ˜¯å¿ƒè‚Œå—æŸæ—¶é‡Šæ”¾çš„å¿ƒè„é…¶ï¼Œé€šå¸¸è¡¨æ˜å‘ç”Ÿäº†å¿ƒè‚Œæ¢—æ­»ã€‚å¦å¤–ï¼Œæ‚£è€…è¿˜å‡ºç°äº†**å¿ƒåŠ¨è¿‡é€Ÿ**ï¼Œå³å¿ƒè·³æ¯”å¹³å¸¸å¿«ã€‚åœ¨å¿ƒè‚Œæ¢—æ­»æ—¶ï¼Œå¿ƒè„å¯èƒ½ä¼šåŠ é€Ÿè·³åŠ¨ï¼Œä»¥è¯•å›¾é€šè¿‡å¢åŠ å¿ƒè„è¾“å‡ºé‡æ¥è¡¥å¿è¢«é˜»å¡çš„å† çŠ¶åŠ¨è„‰ã€‚\n",
    "\n",
    "è€ƒè™‘åˆ°å† çŠ¶åŠ¨è„‰ï¼Œ**å·¦ä¸»å† çŠ¶åŠ¨è„‰**ï¼ˆLMCAï¼‰ä¸ºæ•´ä¸ªå·¦ä¾§å¿ƒè„æä¾›è¡€æ¶²ï¼ŒåŒ…æ‹¬å·¦å¿ƒå®¤ï¼Œè€Œå·¦å¿ƒå®¤æ˜¯ä¸€ä¸ªå…³é”®çš„æ³µè¡€è‚Œè‚‰ã€‚å¦‚æœè¿™é‡Œå‘ç”Ÿå µå¡ï¼Œå¯èƒ½å¯¼è‡´æ›´ä¸¥é‡çš„å¿ƒè‚Œæ¢—æ­»ï¼Œå› ä¸ºå·¦å¿ƒå®¤è‡³å…³é‡è¦ã€‚**å³å† çŠ¶åŠ¨è„‰**ä¸ºå³å¿ƒå®¤å’Œå·¦å¿ƒå®¤ä¸‹å£æä¾›è¡€æ¶²ï¼Œè¿™é‡Œçš„å µå¡ä¹Ÿæ˜¯å¯èƒ½çš„ï¼Œä½†**å·¦ä¸»å† çŠ¶åŠ¨è„‰**é€šå¸¸ä¸ä¸Šè¿°ç—‡çŠ¶æ›´ç›¸å…³ï¼Œå°¤å…¶æ˜¯å½“è‚Œé’™è›‹ç™½å‡é«˜æ—¶ã€‚\n",
    "\n",
    "æ‰€ä»¥ï¼Œå°†æ‰€æœ‰å› ç´ ç»¼åˆè€ƒè™‘ï¼Œæœ€å¯èƒ½å—ç´¯çš„å† çŠ¶åŠ¨è„‰æ˜¯**å·¦ä¸»å† çŠ¶åŠ¨è„‰**ï¼ˆLMCAï¼‰ã€‚æ‚£è€…çš„ç—…å²ã€è‚Œé’™è›‹ç™½å‡é«˜ä»¥åŠå…¸å‹çš„èƒ¸ç—›æ”¾å°„ç—‡çŠ¶éƒ½æŒ‡å‘äº†è¿™ä¸€åŠ¨è„‰ä½œä¸ºç½ªé­ç¥¸é¦–ã€‚\n",
    "\n",
    "\\</think>\n",
    "\n",
    "æœ€å¯èƒ½å—ç´¯çš„å† çŠ¶åŠ¨è„‰æ˜¯**å·¦ä¸»å† çŠ¶åŠ¨è„‰ï¼ˆLMCAï¼‰**ã€‚\n",
    "\n",
    "**è§£é‡Šï¼š**\n",
    "- **ç—‡çŠ¶ï¼š** æ‚£è€…çªå‘èƒ¸ç—›å¹¶æ”¾å°„è‡³é¢ˆéƒ¨å’Œå·¦è‡‚ï¼Œä»¥åŠè‚Œé’™è›‹ç™½å‡é«˜ï¼Œæç¤ºæ€¥æ€§å† çŠ¶åŠ¨è„‰ç»¼åˆç—‡ï¼Œå¯èƒ½æ˜¯å¿ƒè‚Œæ¢—æ­»ã€‚\n",
    "- **ç—…å²ï¼š** é«˜èƒ†å›ºé†‡è¡€ç—‡å’Œå† çŠ¶åŠ¨è„‰ç–¾ç—…ç—…å²æ˜¯åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–çš„é£é™©å› ç´ ï¼Œå¯èƒ½å¯¼è‡´å† çŠ¶åŠ¨è„‰å µå¡ã€‚\n",
    "- **å¿ƒåŠ¨è¿‡é€Ÿï¼š** å¿ƒç‡å¢åŠ å¯èƒ½æ˜¯å¿ƒè„ä¸ºè¡¥å¿å¿ƒè‚Œè¡€æµå‡å°‘è€Œäº§ç”Ÿçš„ååº”ã€‚\n",
    "- **å† çŠ¶åŠ¨è„‰è€ƒè™‘ï¼š** å·¦ä¸»å† çŠ¶åŠ¨è„‰ä¾›åº”å·¦å¿ƒå®¤ï¼Œè¿™ä¸ªè‚Œè‚‰å¯¹å¿ƒè„åŠŸèƒ½è‡³å…³é‡è¦ã€‚ä¸å³å† çŠ¶åŠ¨è„‰ç›¸æ¯”ï¼Œå·¦ä¸»å† çŠ¶åŠ¨è„‰çš„å µå¡ä¼šå¯¼è‡´æ›´ä¸¥é‡ä¸”å±åŠç”Ÿå‘½çš„å¿ƒè‚Œæ¢—æ­»ï¼Œå³å† çŠ¶åŠ¨è„‰é€šå¸¸ä¾›åº”çš„æ˜¯ä¸é‚£ä¹ˆå…³é”®çš„åŒºåŸŸã€‚\n",
    "\n",
    "å› æ­¤ï¼Œç—‡çŠ¶ã€è‚Œé’™è›‹ç™½å‡é«˜ä»¥åŠæ‚£è€…çš„ç—…å²å¼ºçƒˆæŒ‡å‘**å·¦ä¸»å† çŠ¶åŠ¨è„‰**ï¼ˆLMCAï¼‰ä½œä¸ºæœ€å¯èƒ½çš„ç½ªé­ç¥¸é¦–ã€‚<ï½œendâ–ofâ–sentenceï½œ>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acf498-88f5-47b7-831e-2dcbcb96e589",
   "metadata": {},
   "source": [
    "æ ‡å‡†ç­”æ¡ˆï¼š\n",
    "\n",
    "æ ¹æ®æ‚£è€…è¡¨ç°å‡ºçš„çªç„¶èƒ¸ç—›å¹¶æ”¾å°„è‡³é¢ˆéƒ¨å’Œå·¦è‡‚ï¼Œç»“åˆå…¶æœ‰é«˜èƒ†å›ºé†‡è¡€ç—‡å’Œå† çŠ¶åŠ¨è„‰ç–¾ç—…çš„ç—…å²ï¼Œè‚Œé’™è›‹ç™½å‡é«˜å’Œå¿ƒåŠ¨è¿‡é€Ÿï¼Œä¸´åºŠç—‡çŠ¶å¼ºçƒˆæç¤ºå·¦å‰é™æ”¯ï¼ˆLADï¼‰åŠ¨è„‰å—ç´¯ã€‚è¯¥åŠ¨è„‰é€šå¸¸æ˜¯å¼•å‘æ­¤ç±»ç—‡çŠ¶çš„ç½ªé­ç¥¸é¦–ï¼Œå› ä¸ºå®ƒä¾›åº”äº†å¿ƒè„çš„å¤§éƒ¨åˆ†åŒºåŸŸã€‚æ”¾å°„æ€§ç–¼ç—›å’Œè‚Œé’™è›‹ç™½å‡é«˜çš„ç»„åˆè¡¨æ˜å¿ƒè‚Œå—æŸï¼Œè¿™ä½¿å¾—LADæˆä¸ºæœ€å¯èƒ½çš„è‡´ç—…åŠ¨è„‰ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰è¿›ä¸€æ­¥çš„è¯Šæ–­æ£€æŸ¥ï¼ˆå¦‚å¿ƒç”µå›¾ï¼‰çš„æƒ…å†µä¸‹ï¼Œæœ€ç»ˆçš„ç¡®è¯Šä»éœ€ç­‰å¾…ç¡®è®¤ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7cdfee-2b25-4fdc-9ecd-ac5ea4a30e41",
   "metadata": {},
   "source": [
    "èƒ½å¤Ÿçœ‹å‡ºï¼Œåœ¨åŸå§‹çŠ¶æ€ä¸‹ï¼Œæ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ¨ç†å¹¶ç»™å‡ºå›å¤ï¼Œä½†å®é™…ä¸Šç¬¬ä¸€ä¸ªå›ç­”è¿‡ç¨‹å¹¶ä¸ç¬¦åˆåŒ»å­¦è§„èŒƒï¼Œè€Œç¬¬äºŒä¸ªé—®é¢˜åˆ™ç›´æ¥å›ç­”é”™è¯¯ã€‚ç”±æ­¤å¯è§ï¼Œåœ¨åˆå§‹çŠ¶æ€ä¸‹ï¼Œæ¨¡å‹å¯¹äºmedical-o1-reasoning-SFTæ•°æ®é›†é—®ç­”æ•ˆæœå¹¶ä¸å¥½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2dc95-c4c8-4a03-acb9-b8c5bce92e9d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æ¥ä¸‹æ¥å°è¯•è¿›è¡Œå¾®è°ƒï¼Œå¹¶æµ‹è¯•å¾®è°ƒåæ¨¡å‹é—®ç­”æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e5d96-c090-4d06-b133-c36b789e41b8",
   "metadata": {},
   "source": [
    "### ä¸‰ã€æœ€å°å¯è¡Œæ€§å®éªŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6842985-e49e-44f3-9fa5-2e2ba3ec4a3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æ¥ä¸‹æ¥æˆ‘ä»¬å°è¯•è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå¯¹äºå½“å‰æ•°æ®é›†è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥å¸¦å…¥åŸå§‹æ•°æ®é›†çš„éƒ¨åˆ†æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿå¯ä»¥å¸¦å…¥å…¨éƒ¨æ•°æ®å¹¶éå†å¤šæ¬¡è¿›è¡Œå¾®è°ƒã€‚å¯¹äºå¤§å¤šæ•°çš„å¾®è°ƒå®éªŒï¼Œæˆ‘ä»¬éƒ½å¯ä»¥ä»æœ€å°å¯è¡Œæ€§å®éªŒå…¥æ‰‹è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿå°±æ˜¯å…ˆå°è¯•å¸¦å…¥å°‘é‡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¹¶è§‚æµ‹å¾®è°ƒæ•ˆæœã€‚è‹¥å¾®è°ƒå¯ä»¥é¡ºåˆ©æ‰§è¡Œï¼Œå¹¶èƒ½å¤Ÿè·å¾—å¾®è°ƒæ•ˆæœï¼Œå†è€ƒè™‘å¸¦å…¥æ›´å¤šçš„æ•°æ®è¿›è¡Œæ›´å¤§è§„æ¨¡å¾®è°ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672971b-0d59-4c5a-9461-fa7e4c8e1cb4",
   "metadata": {},
   "source": [
    "#### 1.æ•°æ®é›†å‡†å¤‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dc132-747d-40ec-9269-f315678b29e0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä»huggingfaceä¸Šä¸‹è½½medical-o1-reasoning-SFTæ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b548-4e41-406b-8d52-110509156eb9",
   "metadata": {},
   "source": [
    "- è®¾ç½®ä»£ç†ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ac067-8d97-4999-9a3e-1ef305751f39",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ç”±äºhuggingfaceç½‘ç»œå—é™ï¼Œä¸‹è½½æ•°æ®é›†å‰éœ€è¦å…ˆè¿›è¡Œç½‘ç»œç¯å¢ƒè®¾ç½®ã€‚è‹¥æ˜¯AutoDLæœåŠ¡å™¨ï¼Œåˆ™å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹å¼å¼€å¯å­¦æœ¯åŠ é€Ÿï¼Œä»è€Œé¡ºåˆ©è¿æ¥huggingfaceå¹¶è¿›è¡Œæ•°æ®é›†ä¸‹è½½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "281d6b32-4b56-4184-80da-202d7dd589af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef1f84-74f4-4779-9326-a11502aa6ae1",
   "metadata": {},
   "source": [
    "- ä¸‹è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d4f6f-38ce-438e-be64-951f4b907fd7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æ¥ä¸‹æ¥ä½¿ç”¨datasetsè¿›è¡Œæ•°æ®é›†ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5924a688-b61e-4d6d-bc2e-fa0b8c013ae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4635bd-5d5d-406b-900d-eccbfaf261a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90685ac-6510-4fd4-9c2d-def17e0dc6d5",
   "metadata": {},
   "source": [
    "å†æ¬¡ç¡®è®¤æç¤ºè¯æ¨¡æ¿ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acef18a3-d669-4acf-8da7-4da22b3aaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53db946-57c9-40c0-aded-1ac2462f12ec",
   "metadata": {},
   "source": [
    "ç„¶åæå–å¹¶è®¾ç½®æ–‡æœ¬ç”Ÿæˆç»“æŸçš„æ ‡è®°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c75cc9-61f4-4ff3-9124-34ff78456a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117c824-4486-4b17-bbd0-cb3ff9a34f4d",
   "metadata": {},
   "source": [
    "ç„¶åå®šä¹‰å‡½æ•°ï¼Œç”¨äºå¯¹medical-o1-reasoning-SFTæ•°æ®é›†è¿›è¡Œä¿®æ”¹ï¼ŒComplex_CoTåˆ—å’ŒResponseåˆ—è¿›è¡Œæ‹¼æ¥ï¼Œå¹¶åŠ ä¸Šæ–‡æœ¬ç»“æŸæ ‡è®°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dee28cc-1fe8-4c79-860b-c7792b269530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b85065-f6a9-4bff-ae58-cdd940bb2cbf",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250206180316919.png\" alt=\"image-20250206180316919\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4456066-89d8-4ad0-9a2c-edf72eca77f9",
   "metadata": {},
   "source": [
    "åœ¨æœ€å°å¯è¡Œæ€§å®éªŒä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åªä¸‹è½½500æ¡æ•°æ®è¿›è¡Œå¾®è°ƒå³å¯çœ‹å‡ºæ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c18961a1-0db6-404f-81ad-182b2b62a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85106a25-688e-447f-b411-11cc1b0206df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?',\n",
       " 'Complex_CoT': \"Okay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\",\n",
       " 'Response': 'Cystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca9256-067a-4f7b-a339-e0473bc4a81e",
   "metadata": {},
   "source": [
    "ç„¶åè¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82c249c5-ec25-448d-8045-46042d9ad963",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc62f8-96f2-4905-b0e6-c28a60954ce7",
   "metadata": {},
   "source": [
    "å°†æ•°æ®é›†æ•´ç†ä¸ºå¦‚ä¸‹å½¢å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f34f952-656e-44d6-a778-8a70690d4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<|im_end|>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478885f-5a4b-431c-b705-6faea5bcf087",
   "metadata": {},
   "source": [
    "- æ•°æ®é›†ä¿å­˜åœ°å€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ca9f9-34ed-45d8-be58-b4ed829dbcbf",
   "metadata": {},
   "source": [
    "é»˜è®¤æƒ…å†µä¸‹æ•°æ®é›†ä¿å­˜åœ¨ä¸»ç›®å½•ä¸‹.cacheæ–‡ä»¶å¤¹ä¸­ï¼Œæ•°æ®æ–‡ä»¶æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a301ee-a527-4d47-8074-6bd61f5a4bad",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311161225502.png\" alt=\"image-20250311161225502\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a4418-2f7b-4b3e-9efa-efa2a5fd6dd7",
   "metadata": {},
   "source": [
    "#### 2.å¼€å¯å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7956b-039b-483f-9b50-15caf056dfb9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ç„¶åå³å¯æŠŠæ¨¡å‹è®¾ç½®ä¸ºå¾®è°ƒæ¨¡å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028f9bf9-c089-4546-af8e-9eb56ca31b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.9 patched 64 layers with 64 QKV layers, 64 O layers and 64 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d5b06-50b4-4d78-a902-e2df87221f3f",
   "metadata": {},
   "source": [
    "ç„¶åå¯¼å…¥ç›¸å…³çš„åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d135473-1c60-4555-9a14-8abd5e5cabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e168c5-df66-4a26-b65d-ad5251db7e76",
   "metadata": {},
   "source": [
    "åˆ›å»ºæœ‰ç›‘ç£å¾®è°ƒå¯¹è±¡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e1622f1-a0d9-495e-8162-8623027d4a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1e723b8f2c48ea96e9666063e15323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing to [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836f8b5-b608-4af2-be5c-c1a4d4de74c5",
   "metadata": {},
   "source": [
    "è¿™æ®µä»£ç ä¸»è¦æ˜¯ç”¨ **`SFTTrainer`** è¿›è¡Œ **ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰**ï¼Œé€‚ç”¨äº `transformers` å’Œ `Unsloth` ç”Ÿæ€ä¸­çš„æ¨¡å‹å¾®è°ƒï¼š\n",
    "**1. å¯¼å…¥ç›¸å…³åº“**\n",
    "- **`SFTTrainer`**ï¼ˆæ¥è‡ª `trl` åº“ï¼‰ï¼š  \n",
    "  - `trl`ï¼ˆTransformer Reinforcement Learningï¼‰æ˜¯ Hugging Face æ——ä¸‹çš„ `trl` åº“ï¼Œæä¾› **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰** å’Œ **å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰** ç›¸å…³çš„åŠŸèƒ½ã€‚\n",
    "  - `SFTTrainer` ä¸»è¦ç”¨äº **æœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰**ï¼Œé€‚ç”¨äº `LoRA` ç­‰ä½ç§©é€‚é…å¾®è°ƒæ–¹å¼ã€‚\n",
    "\n",
    "- **`TrainingArguments`**ï¼ˆæ¥è‡ª `transformers` åº“ï¼‰ï¼š  \n",
    "  - è¿™ä¸ªç±»ç”¨äºå®šä¹‰ **è®­ç»ƒè¶…å‚æ•°**ï¼Œæ¯”å¦‚æ‰¹é‡å¤§å°ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒæ­¥æ•°ç­‰ã€‚\n",
    "\n",
    "- **`is_bfloat16_supported()`**ï¼ˆæ¥è‡ª `unsloth`ï¼‰ï¼š  \n",
    "  - è¿™ä¸ªå‡½æ•°æ£€æŸ¥ **å½“å‰ GPU æ˜¯å¦æ”¯æŒ `bfloat16`ï¼ˆBF16ï¼‰**ï¼Œå¦‚æœæ”¯æŒï¼Œåˆ™è¿”å› `True`ï¼Œå¦åˆ™è¿”å› `False`ã€‚\n",
    "  - `bfloat16` æ˜¯ä¸€ç§æ›´é«˜æ•ˆçš„æ•°å€¼æ ¼å¼ï¼Œåœ¨ **æ–°æ¬¾ NVIDIA A100/H100** ç­‰ GPU ä¸Šè¡¨ç°æ›´ä¼˜ã€‚\n",
    "\n",
    "**2. åˆå§‹åŒ– `SFTTrainer` è¿›è¡Œæ¨¡å‹å¾®è°ƒ**\n",
    "\n",
    "##### **å‚æ•°è§£æ**\n",
    "##### **â‘  `SFTTrainer` éƒ¨åˆ†**\n",
    "| å‚æ•° | ä½œç”¨ |\n",
    "|------|------|\n",
    "| `model=model` | æŒ‡å®šéœ€è¦è¿›è¡Œå¾®è°ƒçš„ **é¢„è®­ç»ƒæ¨¡å‹** |\n",
    "| `tokenizer=tokenizer` | æŒ‡å®š **åˆ†è¯å™¨**ï¼Œç”¨äºå¤„ç†æ–‡æœ¬æ•°æ® |\n",
    "| `train_dataset=dataset` | ä¼ å…¥ **è®­ç»ƒæ•°æ®é›†** |\n",
    "| `dataset_text_field=\"text\"` | æŒ‡å®šæ•°æ®é›†ä¸­å“ªä¸€åˆ—åŒ…å« **è®­ç»ƒæ–‡æœ¬**ï¼ˆåœ¨ `formatting_prompts_func` é‡Œå¤„ç†ï¼‰ |\n",
    "| `max_seq_length=max_seq_length` | **æœ€å¤§åºåˆ—é•¿åº¦**ï¼Œæ§åˆ¶è¾“å…¥æ–‡æœ¬çš„æœ€å¤§ Token æ•°é‡ |\n",
    "| `dataset_num_proc=2` | **æ•°æ®åŠ è½½çš„å¹¶è¡Œè¿›ç¨‹æ•°**ï¼Œæé«˜æ•°æ®é¢„å¤„ç†æ•ˆç‡ |\n",
    "\n",
    "##### **â‘¡ `TrainingArguments` éƒ¨åˆ†**\n",
    "| å‚æ•° | ä½œç”¨ |\n",
    "|------|------|\n",
    "| `per_device_train_batch_size=2` | æ¯ä¸ª **GPU/è®¾å¤‡** çš„è®­ç»ƒæ‰¹é‡å¤§å°ï¼ˆè¾ƒå°å€¼é€‚åˆå¤§æ¨¡å‹ï¼‰ |\n",
    "| `gradient_accumulation_steps=4` | **æ¢¯åº¦ç´¯ç§¯æ­¥æ•°**ï¼ˆç›¸å½“äº `batch_size=2 Ã— 4 = 8`ï¼‰ |\n",
    "| `warmup_steps=5` | **é¢„çƒ­æ­¥æ•°**ï¼ˆåˆå§‹é˜¶æ®µå­¦ä¹ ç‡è¾ƒä½ï¼Œç„¶åé€æ­¥å‡é«˜ï¼‰ |\n",
    "| `max_steps=60` | **æœ€å¤§è®­ç»ƒæ­¥æ•°**ï¼ˆæ§åˆ¶è®­ç»ƒçš„æ€»æ­¥æ•°ï¼Œæ­¤å¤„æ€»å…±çº¦æ¶ˆè€—60*8=480æ¡æ•°æ®ï¼‰ |\n",
    "| `learning_rate=2e-4` | **å­¦ä¹ ç‡**ï¼ˆ`2e-4` = 0.0002ï¼Œæ§åˆ¶æƒé‡æ›´æ–°å¹…åº¦ï¼‰ |\n",
    "| `fp16=not is_bfloat16_supported()` | å¦‚æœ **GPU ä¸æ”¯æŒ `bfloat16`ï¼Œåˆ™ä½¿ç”¨ `fp16`ï¼ˆ16ä½æµ®ç‚¹æ•°ï¼‰** |\n",
    "| `bf16=is_bfloat16_supported()` | å¦‚æœ **GPU æ”¯æŒ `bfloat16`ï¼Œåˆ™å¯ç”¨ `bfloat16`ï¼ˆè®­ç»ƒæ›´ç¨³å®šï¼‰** |\n",
    "| `logging_steps=10` | **æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—** |\n",
    "| `optim=\"adamw_8bit\"` | **ä½¿ç”¨ `adamw_8bit`ï¼ˆ8-bit AdamWä¼˜åŒ–å™¨ï¼‰å‡å°‘æ˜¾å­˜å ç”¨** |\n",
    "| `weight_decay=0.01` | **æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰**ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |\n",
    "| `lr_scheduler_type=\"linear\"` | **å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥**ï¼ˆçº¿æ€§è¡°å‡ï¼‰ |\n",
    "| `seed=3407` | **éšæœºç§å­**ï¼ˆä¿è¯å®éªŒç»“æœå¯å¤ç°ï¼‰ |\n",
    "| `output_dir=\"outputs\"` | **è®­ç»ƒç»“æœçš„è¾“å‡ºç›®å½•** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cc26d-f6c9-4979-bb12-9c07849bcf06",
   "metadata": {},
   "source": [
    "ç„¶åè®¾ç½®wandbï¼ˆå¯é€‰ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "536a41b7-50bf-4f0f-8240-777b3c80f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5caee957-cc73-46b8-845f-e6119d94f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2323365771\u001b[0m (\u001b[33m2323365771-ff\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"YOUR_WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3d7783c-e1a9-42e0-b441-7673ea0d6d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.8s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/wandb/run-20250311_161713-8wqmaye1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset/runs/8wqmaye1' target=\"_blank\">wise-flower-1</a></strong> to <a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset' target=\"_blank\">https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset/runs/8wqmaye1' target=\"_blank\">https://wandb.ai/2323365771-ff/Fine-tune-QwQ-32B-4bit%20on%20Medical%20COT%20Dataset/runs/8wqmaye1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Fine-tune-QwQ-32B-4bit on Medical COT Dataset', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea7966-4da4-463e-b4c4-cf628b74f3d2",
   "metadata": {},
   "source": [
    "ç„¶åå¼€å§‹å¾®è°ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd782462-3d29-490c-9d63-4cb43a240883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 134,217,728/18,388,423,680 (0.73% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:32, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.179900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1278f09-4a99-4833-900a-659cd3e002eb",
   "metadata": {},
   "source": [
    "æ­¤æ—¶wandbä¸­æ˜¾ç¤ºå†…å®¹å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107a62e-eed6-49ad-8c65-7b51b12819ba",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311162939444.png\" alt=\"image-20250311162939444\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05245a7d-6239-436d-847d-5d3188e1ec52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.2699724833170574, metrics={'train_runtime': 665.1344, 'train_samples_per_second': 1.443, 'train_steps_per_second': 0.09, 'total_flos': 1.740613665982464e+17, 'train_loss': 1.2699724833170574})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa75b2-0345-4c18-8eda-ebf1241b269b",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Œunslothåœ¨å¾®è°ƒç»“æŸåï¼Œä¼šè‡ªåŠ¨æ›´æ–°æ¨¡å‹æƒé‡ï¼ˆåœ¨ç¼“å­˜ä¸­ï¼‰ï¼Œå› æ­¤æ— éœ€æ‰‹åŠ¨åˆå¹¶æ¨¡å‹æƒé‡å³å¯ç›´æ¥è°ƒç”¨å¾®è°ƒåçš„æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efaf4cf4-ef13-4e3f-bc7f-d08fd36ffc4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (1-3): 3 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (4-5): 2 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (6-42): 37 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (43): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (44-59): 16 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (60): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (61): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (62): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "          (63): Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=27648, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=27648, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "64bca6ee-fc4c-4ca0-ab9d-d099f628f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bbb2f57-8e83-484f-b8b6-e34d9d9a529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's think this through. We've got this woman, 61 years old, and she's been dealing with involuntary urine leakage for a while, especially during activities like coughing or sneezing. That's classic stress urinary incontinence, right? It usually happens when there's some pressure on the bladder from things like laughing or lifting something heavy.\n",
      "\n",
      "Now, she's been checked out by a gynecologist and they did a Q-tip test. From what I remember, the Q-tip test is all about figuring out how well the urethra is supported. If the angle is too steep, that could mean the urethra isn't supported properly, which is a sign of stress incontinence.\n",
      "\n",
      "So, if she's got stress urinary incontinence, what does that say about her bladder's behavior? Stress incontinence is mainly about the urethral support and not about the bladder muscle itself. That means the bladder muscle, or the detrusor, isn't really misbehaving here. It should be calm and not causing any issues.\n",
      "\n",
      "Now, let's talk about residual volume. Residual volume is the urine left in the bladder after someone goes. In stress incontinence, since it's not a problem with the bladder muscle, you'd expect the bladder to empty normally. So, there shouldn't be much urine left behind, which means the residual volume should be low.\n",
      "\n",
      "And what about the detrusor contractions? Since the issue is with the urethral support and not the bladder muscle, the detrusor should be relaxed and not causing any unwanted contractions. So, during a cystometry test, they should see that the detrusor is pretty chill.\n",
      "\n",
      "Putting it all together, if she's got stress urinary incontinence, the cystometry should show a low residual volume and no detrusor contractions. That makes sense because it's all about the urethra's support, not the bladder's activity.\n",
      "</think>\n",
      "Based on the symptoms and findings consistent with stress urinary incontinence, cystometry would most likely reveal a low residual volume and no detrusor contractions. Stress urinary incontinence typically involves issues with urethral support rather than bladder muscle activity, so the detrusor muscle should remain relaxed, and the bladder should empty normally, leaving little residual urine.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46783905-93f7-4886-9920-2ca46d836593",
   "metadata": {},
   "source": [
    "æµ‹è¯•ç¬¬äºŒä¸ªé—®é¢˜é—®ç­”æ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b850a79-eba4-44fc-98d8-1488a85742a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1834b90-c3e2-4a0b-b967-0293726a78a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's think about this. The patient is having chest pain that's suddenly come on, and it's radiating to the neck and left arm. That sounds like classic angina, maybe even a heart attack. The fact that it's radiating is a big clueâ€”it often means it's coming from the heart, not something else.\n",
      "\n",
      "Now, the patient has a history of hypercholesterolemia and coronary artery disease. So, we know they've got some pre-existing issues with their heart. They're also showing elevated troponin I levels, which is a sign of heart muscle damage. That really makes me think they're having a heart attack, or myocardial infarction.\n",
      "\n",
      "And then there's the tachycardia. That's a fast heart rate, which could be a reaction to the stress of a heart attack or just the pain itself.\n",
      "\n",
      "Now, let's think about which coronary artery is likely involved. The left anterior descending artery, or LAD, is really important because it supplies a big chunk of the heart, including the anterior wall and the septum. If it gets blocked, it can cause a massive heart attack, and the pain would definitely radiate to the neck and left arm.\n",
      "\n",
      "So, putting it all together, the sudden chest pain, the history of coronary artery disease, the elevated troponin, and the radiation pattern all point to the LAD being the culprit. It's the most logical conclusion given all these signs.\n",
      "</think>\n",
      "Based on the patient's symptoms and medical history, the most likely coronary artery involved in this scenario is the left anterior descending artery (LAD). The LAD is a major coronary artery that supplies blood to the anterior wall of the heart and the interventricular septum. A blockage in the LAD can lead to significant heart muscle damage, which aligns with the elevated troponin I levels indicating myocardial infarction. The sudden-onset chest pain radiating to the neck and left arm, along with the history of coronary artery disease and hypercholesterolemia, further supports this conclusion.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb726b1-1e9a-45e3-a197-992e67e1e5b3",
   "metadata": {},
   "source": [
    "> æ­¤æ—¶æ¨¡å‹è®¤ä¸ºâ€œå·¦ä¸»å† çŠ¶åŠ¨è„‰æœ€å¯èƒ½æ˜¯è¯¥æ‚£è€…ç—‡çŠ¶çš„ç½ªé­ç¥¸é¦–â€ï¼Œä½†å®é™…ä¸Šåº”è¯¥æ˜¯â€œå·¦å‰é™æ”¯ï¼ˆLADï¼‰åŠ¨è„‰å—ç´¯â€å¯¼è‡´ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7c4e0-d1cd-4fc9-8dbd-c76ed581f2b3",
   "metadata": {},
   "source": [
    "èƒ½å¤Ÿå‘ç°ï¼Œç¬¬ä¸€ä¸ªé—®é¢˜å›ç­”æ›´åŠ è§„èŒƒï¼Œå¹¶ä¸”å›ç­”æ­£ç¡®ã€‚ä½†ç¬¬äºŒä¸ªé—®é¢˜ä»ç„¶å›ç­”é”™è¯¯ã€‚ç”±æ­¤å¯ä»¥è€ƒè™‘ç»§ç»­è¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒã€‚ä¸è¿‡åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç°åœ¨å°è§„æ¨¡å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæœ¬åœ°ä¿å­˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83871526-b408-4090-8ea5-73ed0a7daba7",
   "metadata": {},
   "source": [
    "#### 3.æ¨¡å‹åˆå¹¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b846d-ebe6-4b9a-a5ff-e5971258e3fc",
   "metadata": {},
   "source": [
    "æ­¤æ—¶æœ¬åœ°ä¿å­˜çš„æ¨¡å‹æƒé‡åœ¨`outputs`æ–‡ä»¶å¤¹ä¸­ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378c031-80f1-4eac-9775-7557188981ab",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311163146066.png\" alt=\"image-20250311163146066\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e9f53-0ee6-4830-86b9-4ba2f04c5169",
   "metadata": {},
   "source": [
    "ç„¶åå¯ä½¿ç”¨å¦‚ä¸‹ä»£ç è¿›è¡Œæ¨¡å‹æƒé‡åˆå¹¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74072db4-3de1-480c-845a-cdcde2447483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 657.42 out of 1007.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 36/64 [00:01<00:01, 25.68it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:59<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"QwQ-Medical-COT-Tiny\", tokenizer, save_method = \"merged_4bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3acbd1d4-1715-40c8-b40b-6df5491f54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\"QwQ-Medical-COT-Tiny\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7de938-a6fe-4c43-bae3-f7c6187e8e69",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311165148556.png\" alt=\"image-20250311165148556\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1e223-28fd-45c4-b71c-86f6be768a23",
   "metadata": {},
   "source": [
    "æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹ä»£ç å°†å…¶ä¿å­˜ä¸ºGGUFæ ¼å¼ï¼Œæ–¹ä¾¿ä½¿ç”¨ollamaè¿›è¡Œæ¨ç†ã€‚æœ¬éƒ¨åˆ†å¯¼å‡ºä¸åˆå¹¶éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆçº¦20åˆ†é’Ÿå·¦å³ï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf42284a-0bd1-4eb1-abc3-d89cabe472dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 744.1 out of 1007.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:43<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at QwQ-Medical-COT-Tiny-GGUF into bf16 GGUF format.\n",
      "The output location will be /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: QwQ-Medical-COT-Tiny-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00007-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00008-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.36.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.36.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.37.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.37.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00009-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.38.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.38.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.39.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.39.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.40.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.40.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.41.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.41.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.42.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.42.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.43.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00010-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.43.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.43.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.44.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.44.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.45.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.45.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.46.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.46.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.47.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.47.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.48.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.48.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.48.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.48.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.48.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00011-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.48.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.48.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.48.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.48.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.49.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.49.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.49.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.49.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.50.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.50.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.50.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.50.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.50.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.51.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.51.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.51.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.51.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.51.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.52.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.52.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.52.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.52.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.52.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.53.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.53.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.53.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.53.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.53.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00012-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.53.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.53.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.53.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.53.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.54.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.54.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.54.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.54.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.55.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.55.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.55.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.55.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.55.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.56.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.56.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.56.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.56.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.56.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.57.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.57.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.57.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.57.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.57.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.58.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.58.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.58.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.58.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.58.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00013-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:blk.58.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.58.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.58.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.58.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.59.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.59.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.59.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.59.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.60.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.60.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.60.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.60.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.60.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.61.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.61.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.61.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.61.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.61.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.62.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.62.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.62.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.62.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.62.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.63.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.63.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.63.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.63.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.63.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00014-of-00014.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.63.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.63.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {27648, 5120}\n",
      "INFO:hf-to-gguf:blk.63.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 27648}\n",
      "INFO:hf-to-gguf:blk.63.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 27648\n",
      "INFO:hf-to-gguf:gguf: head count = 40\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- '' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "  {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n",
      "        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n<think>\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf: n_tensors = 771, total_size = 65.5G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65.5G/65.5G [08:58<00:00, 122Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "main: build = 4743 (d07c6213)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf' to '/root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.Q4_K_M.gguf' as Q4_K_M using 352 threads\n",
      "llama_model_loader: loaded meta data with 25 key-value pairs and 771 tensors from /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = QwQ 32B Unsloth Bnb 4bit\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = unsloth-bnb-4bit\n",
      "llama_model_loader: - kv   4:                           general.basename str              = QwQ\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 32B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 64\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  321 tensors\n",
      "llama_model_loader: - type bf16:  450 tensors\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA H800 PCIe, compute capability 9.0, VMM: yes\n",
      "  Device 1: NVIDIA H800 PCIe, compute capability 9.0, VMM: yes\n",
      "[   1/ 771]                        output.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q6_K .. size =  1485.00 MiB ->   609.08 MiB\n",
      "[   2/ 771]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   3/ 771]                    token_embd.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q4_K .. size =  1485.00 MiB ->   417.66 MiB\n",
      "[   4/ 771]                    blk.0.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[   5/ 771]                  blk.0.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[   6/ 771]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   7/ 771]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[   8/ 771]                    blk.0.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[   9/ 771]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  10/ 771]                    blk.0.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  11/ 771]                  blk.0.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  12/ 771]                blk.0.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  13/ 771]                blk.0.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  14/ 771]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  15/ 771]                  blk.0.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  16/ 771]                    blk.1.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  17/ 771]                  blk.1.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  18/ 771]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  19/ 771]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  20/ 771]                    blk.1.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  21/ 771]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  22/ 771]                    blk.1.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  23/ 771]                  blk.1.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  24/ 771]                blk.1.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  25/ 771]                blk.1.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  26/ 771]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  27/ 771]                  blk.1.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  28/ 771]                    blk.2.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  29/ 771]                  blk.2.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  30/ 771]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  31/ 771]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  32/ 771]                    blk.2.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  33/ 771]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  34/ 771]                    blk.2.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  35/ 771]                  blk.2.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  36/ 771]                blk.2.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  37/ 771]                blk.2.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  38/ 771]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  39/ 771]                  blk.2.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  40/ 771]                    blk.3.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  41/ 771]                  blk.3.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  42/ 771]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  43/ 771]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  44/ 771]                    blk.3.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  45/ 771]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  46/ 771]                    blk.3.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  47/ 771]                  blk.3.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  48/ 771]                blk.3.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  49/ 771]                blk.3.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  50/ 771]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  51/ 771]                  blk.3.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  52/ 771]                    blk.4.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  53/ 771]                  blk.4.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  54/ 771]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  55/ 771]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  56/ 771]                    blk.4.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  57/ 771]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  58/ 771]                    blk.4.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  59/ 771]                  blk.4.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  60/ 771]                blk.4.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  61/ 771]                blk.4.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  62/ 771]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  63/ 771]                  blk.4.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  64/ 771]                    blk.5.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  65/ 771]                  blk.5.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  66/ 771]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  67/ 771]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  68/ 771]                    blk.5.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  69/ 771]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  70/ 771]                    blk.5.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  71/ 771]                  blk.5.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  72/ 771]                blk.5.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  73/ 771]                blk.5.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  74/ 771]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  75/ 771]                  blk.5.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  76/ 771]                    blk.6.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  77/ 771]                  blk.6.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  78/ 771]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  79/ 771]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  80/ 771]                    blk.6.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  81/ 771]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  82/ 771]                    blk.6.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  83/ 771]                  blk.6.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  84/ 771]                blk.6.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  85/ 771]                blk.6.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  86/ 771]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  87/ 771]                  blk.6.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  88/ 771]                    blk.7.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  89/ 771]                  blk.7.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  90/ 771]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  91/ 771]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  92/ 771]                    blk.7.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  93/ 771]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  94/ 771]                    blk.7.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[  95/ 771]                  blk.7.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  96/ 771]                blk.7.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[  97/ 771]                blk.7.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[  98/ 771]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[  99/ 771]                  blk.7.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 100/ 771]                    blk.8.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 101/ 771]                  blk.8.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 102/ 771]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 103/ 771]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 104/ 771]                    blk.8.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 105/ 771]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 106/ 771]                    blk.8.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 107/ 771]                  blk.8.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 108/ 771]                blk.8.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 109/ 771]                blk.8.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 110/ 771]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 111/ 771]                  blk.8.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 112/ 771]                    blk.9.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 113/ 771]                  blk.9.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 114/ 771]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 115/ 771]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 116/ 771]                    blk.9.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 117/ 771]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 118/ 771]                    blk.9.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 119/ 771]                  blk.9.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 120/ 771]                blk.9.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 121/ 771]                blk.9.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 122/ 771]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 123/ 771]                  blk.9.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 124/ 771]                   blk.10.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 125/ 771]                 blk.10.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 126/ 771]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 127/ 771]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 128/ 771]                   blk.10.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 129/ 771]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 130/ 771]                   blk.10.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 131/ 771]                 blk.10.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 132/ 771]               blk.10.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 133/ 771]               blk.10.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 134/ 771]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 135/ 771]                 blk.10.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 136/ 771]                   blk.11.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 137/ 771]                 blk.11.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 138/ 771]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 139/ 771]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 140/ 771]                   blk.11.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 141/ 771]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 142/ 771]                   blk.11.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 143/ 771]                 blk.11.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 144/ 771]               blk.11.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 145/ 771]               blk.11.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 146/ 771]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 147/ 771]                 blk.11.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 148/ 771]                   blk.12.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 149/ 771]                 blk.12.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 150/ 771]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 151/ 771]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 152/ 771]                   blk.12.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 153/ 771]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 154/ 771]                   blk.12.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 155/ 771]                 blk.12.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 156/ 771]               blk.12.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 157/ 771]               blk.12.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 158/ 771]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 159/ 771]                 blk.12.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 160/ 771]                   blk.13.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 161/ 771]                 blk.13.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 162/ 771]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 163/ 771]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 164/ 771]                   blk.13.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 165/ 771]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 166/ 771]                   blk.13.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 167/ 771]                 blk.13.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 168/ 771]               blk.13.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 169/ 771]               blk.13.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 170/ 771]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 171/ 771]                 blk.13.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 172/ 771]                   blk.14.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 173/ 771]                 blk.14.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 174/ 771]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 175/ 771]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 176/ 771]                   blk.14.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 177/ 771]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 178/ 771]                   blk.14.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 179/ 771]                 blk.14.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 180/ 771]               blk.14.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 181/ 771]               blk.14.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 182/ 771]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 183/ 771]                 blk.14.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 184/ 771]                   blk.15.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 185/ 771]                 blk.15.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 186/ 771]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 187/ 771]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 188/ 771]                   blk.15.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 189/ 771]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 190/ 771]                   blk.15.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 191/ 771]                 blk.15.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 192/ 771]               blk.15.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 193/ 771]               blk.15.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 194/ 771]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 195/ 771]                 blk.15.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 196/ 771]                   blk.16.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 197/ 771]                 blk.16.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 198/ 771]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 199/ 771]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 200/ 771]                   blk.16.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 201/ 771]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 202/ 771]                   blk.16.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 203/ 771]                 blk.16.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 204/ 771]               blk.16.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 205/ 771]               blk.16.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 206/ 771]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 207/ 771]                 blk.16.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 208/ 771]                   blk.17.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 209/ 771]                 blk.17.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 210/ 771]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 211/ 771]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 212/ 771]                   blk.17.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 213/ 771]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 214/ 771]                   blk.17.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 215/ 771]                 blk.17.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 216/ 771]               blk.17.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 217/ 771]               blk.17.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 218/ 771]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 219/ 771]                 blk.17.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 220/ 771]                   blk.18.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 221/ 771]                 blk.18.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 222/ 771]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 223/ 771]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 224/ 771]                   blk.18.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 225/ 771]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 226/ 771]                   blk.18.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 227/ 771]                 blk.18.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 228/ 771]               blk.18.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 229/ 771]               blk.18.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 230/ 771]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 231/ 771]                 blk.18.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 232/ 771]                   blk.19.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 233/ 771]                 blk.19.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 234/ 771]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 235/ 771]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 236/ 771]                   blk.19.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 237/ 771]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 238/ 771]                   blk.19.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 239/ 771]                 blk.19.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 240/ 771]               blk.19.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 241/ 771]               blk.19.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 242/ 771]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 243/ 771]                 blk.19.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 244/ 771]                   blk.20.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 245/ 771]                 blk.20.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 246/ 771]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 247/ 771]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 248/ 771]                   blk.20.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 249/ 771]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 250/ 771]                   blk.20.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 251/ 771]                 blk.20.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 252/ 771]               blk.20.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 253/ 771]               blk.20.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 254/ 771]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 255/ 771]                 blk.20.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 256/ 771]                   blk.21.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 257/ 771]                 blk.21.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 258/ 771]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 259/ 771]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 260/ 771]                   blk.21.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 261/ 771]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 262/ 771]                   blk.21.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 263/ 771]                 blk.21.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 264/ 771]               blk.21.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 265/ 771]               blk.21.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 266/ 771]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 267/ 771]                 blk.21.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 268/ 771]                   blk.22.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 269/ 771]                 blk.22.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 270/ 771]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 271/ 771]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 272/ 771]                   blk.22.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 273/ 771]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 274/ 771]                   blk.22.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 275/ 771]                 blk.22.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 276/ 771]               blk.22.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 277/ 771]               blk.22.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 278/ 771]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 279/ 771]                 blk.22.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 280/ 771]                   blk.23.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 281/ 771]                 blk.23.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 282/ 771]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 283/ 771]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 284/ 771]                   blk.23.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 285/ 771]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 286/ 771]                   blk.23.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 287/ 771]                 blk.23.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 288/ 771]               blk.23.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 289/ 771]               blk.23.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 290/ 771]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 291/ 771]                 blk.23.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 292/ 771]                   blk.24.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 293/ 771]                 blk.24.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 294/ 771]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 295/ 771]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 296/ 771]                   blk.24.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 297/ 771]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 298/ 771]                   blk.24.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 299/ 771]                 blk.24.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 300/ 771]               blk.24.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 301/ 771]               blk.24.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 302/ 771]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 303/ 771]                 blk.24.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 304/ 771]                   blk.25.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 305/ 771]                 blk.25.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 306/ 771]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 307/ 771]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 308/ 771]                   blk.25.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 309/ 771]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 310/ 771]                   blk.25.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 311/ 771]                 blk.25.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 312/ 771]               blk.25.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 313/ 771]               blk.25.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 314/ 771]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 315/ 771]                 blk.25.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 316/ 771]                   blk.26.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 317/ 771]                 blk.26.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 318/ 771]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 319/ 771]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 320/ 771]                   blk.26.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 321/ 771]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 322/ 771]                   blk.26.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 323/ 771]                 blk.26.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 324/ 771]               blk.26.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 325/ 771]               blk.26.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 326/ 771]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 327/ 771]                 blk.26.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 328/ 771]                   blk.27.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 329/ 771]                 blk.27.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 330/ 771]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 331/ 771]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 332/ 771]                   blk.27.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 333/ 771]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 334/ 771]                   blk.27.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 335/ 771]                 blk.27.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 336/ 771]               blk.27.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 337/ 771]               blk.27.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 338/ 771]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 339/ 771]                 blk.27.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 340/ 771]                   blk.28.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 341/ 771]                 blk.28.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 342/ 771]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 343/ 771]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 344/ 771]                   blk.28.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 345/ 771]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 346/ 771]                   blk.28.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 347/ 771]                 blk.28.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 348/ 771]               blk.28.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 349/ 771]               blk.28.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 350/ 771]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 351/ 771]                 blk.28.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 352/ 771]                   blk.29.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 353/ 771]                 blk.29.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 354/ 771]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 355/ 771]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 356/ 771]                   blk.29.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 357/ 771]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 358/ 771]                   blk.29.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 359/ 771]                 blk.29.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 360/ 771]               blk.29.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 361/ 771]               blk.29.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 362/ 771]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 363/ 771]                 blk.29.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 364/ 771]                   blk.30.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 365/ 771]                 blk.30.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 366/ 771]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 367/ 771]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 368/ 771]                   blk.30.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 369/ 771]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 370/ 771]                   blk.30.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 371/ 771]                 blk.30.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 372/ 771]               blk.30.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 373/ 771]               blk.30.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 374/ 771]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 375/ 771]                 blk.30.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 376/ 771]                   blk.31.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 377/ 771]                 blk.31.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 378/ 771]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 379/ 771]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 380/ 771]                   blk.31.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 381/ 771]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 382/ 771]                   blk.31.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 383/ 771]                 blk.31.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 384/ 771]               blk.31.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 385/ 771]               blk.31.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 386/ 771]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 387/ 771]                 blk.31.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 388/ 771]                   blk.32.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 389/ 771]                 blk.32.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 390/ 771]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 391/ 771]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 392/ 771]                   blk.32.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 393/ 771]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 394/ 771]                   blk.32.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 395/ 771]                 blk.32.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 396/ 771]               blk.32.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 397/ 771]               blk.32.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 398/ 771]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 399/ 771]                 blk.32.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 400/ 771]                   blk.33.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 401/ 771]                 blk.33.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 402/ 771]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 403/ 771]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 404/ 771]                   blk.33.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 405/ 771]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 406/ 771]                   blk.33.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 407/ 771]                 blk.33.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 408/ 771]               blk.33.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 409/ 771]               blk.33.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 410/ 771]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 411/ 771]                 blk.33.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 412/ 771]                   blk.34.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 413/ 771]                 blk.34.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 414/ 771]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 415/ 771]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 416/ 771]                   blk.34.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 417/ 771]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 418/ 771]                   blk.34.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 419/ 771]                 blk.34.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 420/ 771]               blk.34.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 421/ 771]               blk.34.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 422/ 771]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 423/ 771]                 blk.34.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 424/ 771]                   blk.35.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 425/ 771]                 blk.35.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 426/ 771]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 427/ 771]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 428/ 771]                   blk.35.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 429/ 771]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 430/ 771]                   blk.35.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 431/ 771]                 blk.35.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 432/ 771]               blk.35.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 433/ 771]               blk.35.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 434/ 771]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 435/ 771]                 blk.35.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 436/ 771]                   blk.36.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 437/ 771]                 blk.36.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 438/ 771]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 439/ 771]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 440/ 771]                   blk.36.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 441/ 771]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 442/ 771]                   blk.36.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 443/ 771]                 blk.36.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 444/ 771]               blk.36.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 445/ 771]               blk.36.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 446/ 771]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 447/ 771]                 blk.36.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 448/ 771]                   blk.37.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 449/ 771]                 blk.37.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 450/ 771]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 451/ 771]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 452/ 771]                   blk.37.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 453/ 771]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 454/ 771]                   blk.37.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 455/ 771]                 blk.37.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 456/ 771]               blk.37.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 457/ 771]               blk.37.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 458/ 771]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 459/ 771]                 blk.37.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 460/ 771]                   blk.38.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 461/ 771]                 blk.38.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 462/ 771]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 463/ 771]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 464/ 771]                   blk.38.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 465/ 771]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 466/ 771]                   blk.38.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 467/ 771]                 blk.38.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 468/ 771]               blk.38.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 469/ 771]               blk.38.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 470/ 771]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 471/ 771]                 blk.38.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 472/ 771]                   blk.39.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 473/ 771]                 blk.39.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 474/ 771]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 475/ 771]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 476/ 771]                   blk.39.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 477/ 771]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 478/ 771]                   blk.39.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 479/ 771]                 blk.39.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 480/ 771]               blk.39.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 481/ 771]               blk.39.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 482/ 771]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 483/ 771]                 blk.39.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 484/ 771]                   blk.40.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 485/ 771]                 blk.40.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 486/ 771]              blk.40.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 487/ 771]            blk.40.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 488/ 771]                   blk.40.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 489/ 771]                 blk.40.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 490/ 771]                   blk.40.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 491/ 771]                 blk.40.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 492/ 771]               blk.40.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 493/ 771]               blk.40.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 494/ 771]               blk.40.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 495/ 771]                 blk.40.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 496/ 771]                   blk.41.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 497/ 771]                 blk.41.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 498/ 771]              blk.41.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 499/ 771]            blk.41.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 500/ 771]                   blk.41.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 501/ 771]                 blk.41.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 502/ 771]                   blk.41.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 503/ 771]                 blk.41.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 504/ 771]               blk.41.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 505/ 771]               blk.41.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 506/ 771]               blk.41.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 507/ 771]                 blk.41.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 508/ 771]                   blk.42.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 509/ 771]                 blk.42.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 510/ 771]              blk.42.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 511/ 771]            blk.42.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 512/ 771]                   blk.42.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 513/ 771]                 blk.42.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 514/ 771]                   blk.42.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 515/ 771]                 blk.42.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 516/ 771]               blk.42.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 517/ 771]               blk.42.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 518/ 771]               blk.42.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 519/ 771]                 blk.42.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 520/ 771]                   blk.43.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 521/ 771]                 blk.43.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 522/ 771]              blk.43.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 523/ 771]            blk.43.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 524/ 771]                   blk.43.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 525/ 771]                 blk.43.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 526/ 771]                   blk.43.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 527/ 771]                 blk.43.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 528/ 771]               blk.43.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 529/ 771]               blk.43.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 530/ 771]               blk.43.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 531/ 771]                 blk.43.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 532/ 771]                   blk.44.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 533/ 771]                 blk.44.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 534/ 771]              blk.44.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 535/ 771]            blk.44.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 536/ 771]                   blk.44.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 537/ 771]                 blk.44.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 538/ 771]                   blk.44.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 539/ 771]                 blk.44.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 540/ 771]               blk.44.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 541/ 771]               blk.44.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 542/ 771]               blk.44.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 543/ 771]                 blk.44.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 544/ 771]                   blk.45.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 545/ 771]                 blk.45.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 546/ 771]              blk.45.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 547/ 771]            blk.45.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 548/ 771]                   blk.45.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 549/ 771]                 blk.45.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 550/ 771]                   blk.45.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 551/ 771]                 blk.45.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 552/ 771]               blk.45.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 553/ 771]               blk.45.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 554/ 771]               blk.45.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 555/ 771]                 blk.45.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 556/ 771]                   blk.46.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 557/ 771]                 blk.46.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 558/ 771]              blk.46.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 559/ 771]            blk.46.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 560/ 771]                   blk.46.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 561/ 771]                 blk.46.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 562/ 771]                   blk.46.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 563/ 771]                 blk.46.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 564/ 771]               blk.46.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 565/ 771]               blk.46.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 566/ 771]               blk.46.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 567/ 771]                 blk.46.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 568/ 771]                   blk.47.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 569/ 771]                 blk.47.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 570/ 771]              blk.47.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 571/ 771]            blk.47.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 572/ 771]                   blk.47.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 573/ 771]                 blk.47.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 574/ 771]                   blk.47.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 575/ 771]                 blk.47.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 576/ 771]               blk.47.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 577/ 771]               blk.47.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 578/ 771]               blk.47.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 579/ 771]                 blk.47.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 580/ 771]                   blk.48.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 581/ 771]                 blk.48.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 582/ 771]              blk.48.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 583/ 771]            blk.48.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 584/ 771]                   blk.48.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 585/ 771]                 blk.48.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 586/ 771]                   blk.48.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 587/ 771]                 blk.48.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 588/ 771]               blk.48.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 589/ 771]               blk.48.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 590/ 771]               blk.48.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 591/ 771]                 blk.48.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 592/ 771]                   blk.49.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 593/ 771]                 blk.49.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 594/ 771]              blk.49.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 595/ 771]            blk.49.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 596/ 771]                   blk.49.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 597/ 771]                 blk.49.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 598/ 771]                   blk.49.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 599/ 771]                 blk.49.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 600/ 771]               blk.49.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 601/ 771]               blk.49.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 602/ 771]               blk.49.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 603/ 771]                 blk.49.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 604/ 771]                   blk.50.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 605/ 771]                 blk.50.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 606/ 771]              blk.50.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 607/ 771]            blk.50.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 608/ 771]                   blk.50.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 609/ 771]                 blk.50.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 610/ 771]                   blk.50.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 611/ 771]                 blk.50.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 612/ 771]               blk.50.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 613/ 771]               blk.50.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 614/ 771]               blk.50.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 615/ 771]                 blk.50.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 616/ 771]                   blk.51.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 617/ 771]                 blk.51.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 618/ 771]              blk.51.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 619/ 771]            blk.51.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 620/ 771]                   blk.51.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 621/ 771]                 blk.51.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 622/ 771]                   blk.51.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 623/ 771]                 blk.51.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 624/ 771]               blk.51.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 625/ 771]               blk.51.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 626/ 771]               blk.51.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 627/ 771]                 blk.51.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 628/ 771]                   blk.52.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 629/ 771]                 blk.52.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 630/ 771]              blk.52.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 631/ 771]            blk.52.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 632/ 771]                   blk.52.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 633/ 771]                 blk.52.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 634/ 771]                   blk.52.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 635/ 771]                 blk.52.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 636/ 771]               blk.52.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 637/ 771]               blk.52.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 638/ 771]               blk.52.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 639/ 771]                 blk.52.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 640/ 771]                   blk.53.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 641/ 771]                 blk.53.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 642/ 771]              blk.53.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 643/ 771]            blk.53.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 644/ 771]                   blk.53.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 645/ 771]                 blk.53.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 646/ 771]                   blk.53.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 647/ 771]                 blk.53.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 648/ 771]               blk.53.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 649/ 771]               blk.53.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 650/ 771]               blk.53.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 651/ 771]                 blk.53.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 652/ 771]                   blk.54.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 653/ 771]                 blk.54.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 654/ 771]              blk.54.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 655/ 771]            blk.54.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 656/ 771]                   blk.54.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 657/ 771]                 blk.54.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 658/ 771]                   blk.54.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 659/ 771]                 blk.54.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 660/ 771]               blk.54.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 661/ 771]               blk.54.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 662/ 771]               blk.54.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 663/ 771]                 blk.54.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 664/ 771]                   blk.55.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 665/ 771]                 blk.55.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 666/ 771]              blk.55.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 667/ 771]            blk.55.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 668/ 771]                   blk.55.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 669/ 771]                 blk.55.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 670/ 771]                   blk.55.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 671/ 771]                 blk.55.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 672/ 771]               blk.55.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 673/ 771]               blk.55.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 674/ 771]               blk.55.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 675/ 771]                 blk.55.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 676/ 771]                   blk.56.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 677/ 771]                 blk.56.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 678/ 771]              blk.56.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 679/ 771]            blk.56.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 680/ 771]                   blk.56.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 681/ 771]                 blk.56.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 682/ 771]                   blk.56.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 683/ 771]                 blk.56.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 684/ 771]               blk.56.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 685/ 771]               blk.56.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 686/ 771]               blk.56.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 687/ 771]                 blk.56.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 688/ 771]                   blk.57.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 689/ 771]                 blk.57.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 690/ 771]              blk.57.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 691/ 771]            blk.57.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 692/ 771]                   blk.57.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 693/ 771]                 blk.57.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 694/ 771]                   blk.57.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 695/ 771]                 blk.57.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 696/ 771]               blk.57.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 697/ 771]               blk.57.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 698/ 771]               blk.57.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 699/ 771]                 blk.57.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 700/ 771]                   blk.58.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 701/ 771]                 blk.58.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 702/ 771]              blk.58.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 703/ 771]            blk.58.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 704/ 771]                   blk.58.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 705/ 771]                 blk.58.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 706/ 771]                   blk.58.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 707/ 771]                 blk.58.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 708/ 771]               blk.58.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 709/ 771]               blk.58.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 710/ 771]               blk.58.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 711/ 771]                 blk.58.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 712/ 771]                   blk.59.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 713/ 771]                 blk.59.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 714/ 771]              blk.59.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 715/ 771]            blk.59.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 716/ 771]                   blk.59.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 717/ 771]                 blk.59.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 718/ 771]                   blk.59.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 719/ 771]                 blk.59.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 720/ 771]               blk.59.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 721/ 771]               blk.59.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 722/ 771]               blk.59.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 723/ 771]                 blk.59.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 724/ 771]                   blk.60.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 725/ 771]                 blk.60.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 726/ 771]              blk.60.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 727/ 771]            blk.60.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 728/ 771]                   blk.60.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 729/ 771]                 blk.60.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 730/ 771]                   blk.60.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 731/ 771]                 blk.60.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 732/ 771]               blk.60.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 733/ 771]               blk.60.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 734/ 771]               blk.60.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 735/ 771]                 blk.60.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 736/ 771]                   blk.61.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 737/ 771]                 blk.61.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 738/ 771]              blk.61.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 739/ 771]            blk.61.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 740/ 771]                   blk.61.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 741/ 771]                 blk.61.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 742/ 771]                   blk.61.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 743/ 771]                 blk.61.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 744/ 771]               blk.61.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 745/ 771]               blk.61.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 746/ 771]               blk.61.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 747/ 771]                 blk.61.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 748/ 771]                   blk.62.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 749/ 771]                 blk.62.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 750/ 771]              blk.62.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 751/ 771]            blk.62.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 752/ 771]                   blk.62.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 753/ 771]                 blk.62.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 754/ 771]                   blk.62.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 755/ 771]                 blk.62.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 756/ 771]               blk.62.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 757/ 771]               blk.62.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 758/ 771]               blk.62.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 759/ 771]                 blk.62.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 760/ 771]                   blk.63.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 761/ 771]                 blk.63.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 762/ 771]              blk.63.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 763/ 771]            blk.63.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 764/ 771]                   blk.63.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 765/ 771]                 blk.63.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 766/ 771]                   blk.63.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
      "[ 767/ 771]                 blk.63.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 768/ 771]               blk.63.ffn_down.weight - [27648,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   270.00 MiB ->   110.74 MiB\n",
      "[ 769/ 771]               blk.63.ffn_gate.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "[ 770/ 771]               blk.63.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
      "[ 771/ 771]                 blk.63.ffn_up.weight - [ 5120, 27648,     1,     1], type =   bf16, converting to q4_K .. size =   270.00 MiB ->    75.94 MiB\n",
      "llama_model_quantize_impl: model size  = 62494.27 MB\n",
      "llama_model_quantize_impl: quant size  = 18926.01 MB\n",
      "\n",
      "main: quantize time = 243777.08 ms\n",
      "main:    total time = 243777.08 ms\n",
      "Unsloth: Conversion completed! Output location: /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"QwQ-Medical-COT-Tiny-GGUF\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8fee6-a2c5-46eb-ab28-917c905e101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q8_0\")\n",
    "#model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"f16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34101af6-bc92-4e72-b432-a520b0dd63ce",
   "metadata": {},
   "source": [
    "åˆ›å»ºå®ŒGGUFæ–‡ä»¶åå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dad9da-6b4b-4ff7-8bd4-596d6363eef9",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311173422135.png\" alt=\"image-20250311173422135\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4718d-a720-490e-8032-41edbbc9f16f",
   "metadata": {},
   "source": [
    "- å¾®è°ƒåæ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec9592-a14a-43cf-b9f8-7e55fedebb9b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ä¸ºé¿å…å˜é‡åç§°å†²çªï¼Œè¿™é‡Œéœ€è¦å…ˆé‡å¯Jupyter Kernelï¼Œç„¶åå¾®è°ƒæ¨¡å‹ä¿å­˜å’Œæ¨ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80bf42e-5f1a-47af-b94d-f5444aadaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b5b6b73-7e8c-4631-a777-18f5af25bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa3a395c-1999-445d-b55e-02a1b32d35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H800 PCIe. Num GPUs = 2. Max memory: 79.205 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e02ec6d16b14a53b1e020388ddc14d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/root/autodl-tmp/QwQ-Medical-COT-Tiny\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cc227-7af6-4bd8-aeda-25a10134e3cd",
   "metadata": {},
   "source": [
    "æ­¤æ—¶modelå°±æ˜¯è¯»å–è¿›æ¥æ¨¡å‹å°±æ˜¯å¾®è°ƒåæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d18c6ebd-a376-4fc3-8ac0-4cf2732eb32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b918663c-436e-479e-bee4-36e1dab52af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/root/autodl-tmp/QwQ-Medical-COT-Tiny', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc617495-c550-403f-b97f-be89a22a4e89",
   "metadata": {},
   "source": [
    "ç„¶åè®¾ç½®ä¸ºæ¨ç†æ¨¡å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "816befad-34c7-45bb-a5b6-90420af09aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756eab7-47b0-47e3-b2bd-1ef5c43a6a8d",
   "metadata": {},
   "source": [
    "å¹¶è®¾ç½®æç¤ºè¯æ¨¡æ¿ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9749dac-be0e-422d-9cc3-bc63554d6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style_chat = \"\"\"è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d84917-674e-4a80-b283-ff25e57629dc",
   "metadata": {},
   "source": [
    "ç„¶åå³å¯å¼€å§‹è¿›è¡Œå¯¹è¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85ccd98f-26e5-4702-ab8c-fed2ed3f227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58a793be-d98a-4ef5-9360-6c97135277e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\\n\\n### Question:\\nä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\\n\\n### Response:\\n<think>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prompt_style_chat.format(question, \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0495f257-ec7d-4acd-89a3-4628176f3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_style_chat.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d903bbc3-cd14-4171-9ec7-4e2d1389337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d030ddf-8c50-4164-93af-32c2c86b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23adc604-57de-4f61-915d-2da54f7a18e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\\n\\n### Question:\\nä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\\n\\n### Response:\\n<think>\\nå—¯ï¼Œç”¨æˆ·å‘æ¥â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œæˆ‘éœ€è¦å›åº”ã€‚é¦–å…ˆï¼Œåº”è¯¥è¡¨è¾¾åŒæ ·çš„é—®å€™ï¼Œæ¯”å¦‚â€œä½ å¥½å•Šï¼â€ç„¶åï¼Œç”¨æˆ·æåˆ°â€œå¥½ä¹…ä¸è§â€ï¼Œè¯´æ˜ä¹‹å‰å¯èƒ½æœ‰æ®µæ—¶é—´æ²¡è”ç³»äº†ï¼Œåº”è¯¥å›åº”è¯´è§åˆ°ä»–ä»¬å¾ˆé«˜å…´ï¼Œæˆ–è€…é—®æœ€è¿‘æ€ä¹ˆæ ·ã€‚æ¯”å¦‚å¯ä»¥è¯´â€œå¾ˆé«˜å…´åˆè§åˆ°ä½ ï¼æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿâ€ã€‚è¿™æ ·æ—¢å›åº”äº†å¯¹æ–¹çš„é—®å€™ï¼Œåˆå¼€å¯äº†è¿›ä¸€æ­¥çš„å¯¹è¯ï¼Œè®©ç”¨æˆ·æœ‰æœºä¼šåˆ†äº«è¿‘å†µã€‚å¦å¤–ï¼Œè¦ä¿æŒå‹å¥½å’Œçƒ­æƒ…çš„è¯­æ°”ï¼Œç¬¦åˆåŠ©äººä¸ºä¹åŠ©æ‰‹çš„è§’è‰²ã€‚å¯èƒ½è¿˜è¦æ³¨æ„ä¸è¦é—®å¾—å¤ªæ·±å…¥ï¼Œä¿æŒè‡ªç„¶å’Œè½»æ¾ã€‚æ‰€ä»¥ç»¼åˆèµ·æ¥ï¼Œå›å¤åº”è¯¥æ˜¯ï¼šâ€œä½ å¥½å•Šï¼å¾ˆé«˜å…´åˆè§åˆ°ä½ ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿâ€è¿™æ ·æ—¢å‹å¥½ï¼Œåˆæä¾›äº†å¸®åŠ©çš„å¯èƒ½ï¼Œç¬¦åˆç”¨æˆ·ä½œä¸ºåŠ©æ‰‹çš„è®¾å®šã€‚\\n</think>\\n\\nä½ å¥½å•Šï¼å¾ˆé«˜å…´åˆè§åˆ°ä½ ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ<|im_end|>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe45db40-eac0-4f73-b42a-e29582fda933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å—¯ï¼Œç”¨æˆ·å‘æ¥â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œæˆ‘éœ€è¦å›åº”ã€‚é¦–å…ˆï¼Œåº”è¯¥è¡¨è¾¾åŒæ ·çš„é—®å€™ï¼Œæ¯”å¦‚â€œä½ å¥½å•Šï¼â€ç„¶åï¼Œç”¨æˆ·æåˆ°â€œå¥½ä¹…ä¸è§â€ï¼Œè¯´æ˜ä¹‹å‰å¯èƒ½æœ‰æ®µæ—¶é—´æ²¡è”ç³»äº†ï¼Œåº”è¯¥å›åº”è¯´è§åˆ°ä»–ä»¬å¾ˆé«˜å…´ï¼Œæˆ–è€…é—®æœ€è¿‘æ€ä¹ˆæ ·ã€‚æ¯”å¦‚å¯ä»¥è¯´â€œå¾ˆé«˜å…´åˆè§åˆ°ä½ ï¼æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿâ€ã€‚è¿™æ ·æ—¢å›åº”äº†å¯¹æ–¹çš„é—®å€™ï¼Œåˆå¼€å¯äº†è¿›ä¸€æ­¥çš„å¯¹è¯ï¼Œè®©ç”¨æˆ·æœ‰æœºä¼šåˆ†äº«è¿‘å†µã€‚å¦å¤–ï¼Œè¦ä¿æŒå‹å¥½å’Œçƒ­æƒ…çš„è¯­æ°”ï¼Œç¬¦åˆåŠ©äººä¸ºä¹åŠ©æ‰‹çš„è§’è‰²ã€‚å¯èƒ½è¿˜è¦æ³¨æ„ä¸è¦é—®å¾—å¤ªæ·±å…¥ï¼Œä¿æŒè‡ªç„¶å’Œè½»æ¾ã€‚æ‰€ä»¥ç»¼åˆèµ·æ¥ï¼Œå›å¤åº”è¯¥æ˜¯ï¼šâ€œä½ å¥½å•Šï¼å¾ˆé«˜å…´åˆè§åˆ°ä½ ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿâ€è¿™æ ·æ—¢å‹å¥½ï¼Œåˆæä¾›äº†å¸®åŠ©çš„å¯èƒ½ï¼Œç¬¦åˆç”¨æˆ·ä½œä¸ºåŠ©æ‰‹çš„è®¾å®šã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½å•Šï¼å¾ˆé«˜å…´åˆè§åˆ°ä½ ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a699bd0-928e-4d4d-9216-21006750f907",
   "metadata": {},
   "source": [
    "æ­¤æ—¶æ˜¾å­˜çº¦20Gå·¦å³ï¼š\n",
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311191243266.png\" alt=\"image-20250311191243266\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358b099-2f75-4fb3-b94c-8e64ddbefb63",
   "metadata": {},
   "source": [
    "- transformersæ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc6511-bfd2-41ab-b14b-f27c96fb4fc2",
   "metadata": {},
   "source": [
    "ä¸ºäº†é¿å…é‡å¤è¯»å–å ç”¨æ˜¾å­˜ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥å…ˆé‡å¯kernelï¼Œå†æ‰§è¡Œä»¥ä¸‹ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed598c39-af8a-4e3b-92a7-5665985eac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3431071adbba483fb140fc1c84a5ac55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å—¯ï¼Œç”¨æˆ·è·Ÿæˆ‘è¯´â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œçœ‹èµ·æ¥æŒºå¼€å¿ƒçš„ã€‚æˆ‘å¾—å…ˆå›åº”ä¸€ä¸‹ä»–çš„çƒ­æƒ…ï¼Œæ¯•ç«Ÿå¥½ä¹…æ²¡è”ç³»äº†ï¼Œåº”è¯¥è®©ä»–æ„Ÿå—åˆ°æˆ‘çš„çƒ­æƒ…å’Œå…³å¿ƒã€‚ç„¶åï¼Œæˆ‘å¾—æƒ³ä¸€æƒ³ä»–ä¸ºä»€ä¹ˆä¼šçªç„¶è”ç³»æˆ‘ï¼Œæ˜¯ä¸æ˜¯é‡åˆ°äº†ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿæˆ–è€…ä»–åªæ˜¯æƒ³èŠèŠè¿‘å†µï¼Ÿè¿™æ—¶å€™æˆ‘å¾—ä¿æŒå¼€æ”¾çš„æ€åº¦ï¼Œè®©ä»–çŸ¥é“æˆ‘å¾ˆä¹æ„å¸®å¿™æˆ–è€…å€¾å¬ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘åº”è¯¥é¼“åŠ±ä»–å¤šè¯´ä¸€äº›ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½æ›´å¥½åœ°äº†è§£ä»–çš„éœ€æ±‚ã€‚æ¯”å¦‚é—®ä»–æœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Œæˆ–è€…æœ‰ä»€ä¹ˆæƒ³åˆ†äº«çš„äº‹æƒ…ã€‚ä¸è¿‡ï¼Œæˆ‘å¾—æ³¨æ„åˆ«å¤ªæ€¥ï¼Œè®©ä»–æ„Ÿè§‰è‡ªç„¶ä¸€äº›ã€‚æ¯•ç«Ÿæœ‰æ—¶å€™äººä»¬å¯èƒ½ä¸å¤ªæ„¿æ„ä¸€å¼€å§‹å°±é€éœ²å¤ªå¤šã€‚\n",
      "\n",
      "å¦å¤–ï¼Œæˆ‘å¾—ç¡®ä¿æˆ‘çš„å›ç­”æ—¢å‹å¥½åˆä¸“ä¸šï¼Œä¸èƒ½å¤ªéšä¾¿ä¹Ÿä¸èƒ½å¤ªç”Ÿç¡¬ã€‚è¦è®©ä»–è§‰å¾—æˆ‘ä»¬ä¹‹é—´çš„äº¤æµæ—¢è½»æ¾åˆæœ‰æ•ˆã€‚å¯èƒ½çš„è¯ï¼Œå¯ä»¥ä¸¾å‡ ä¸ªä¾‹å­ï¼Œæ¯”å¦‚ä»–å¯èƒ½éœ€è¦å¸®åŠ©è§£å†³é—®é¢˜ï¼Œæˆ–è€…åªæ˜¯æƒ³éšä¾¿èŠèŠã€‚è¿™æ ·ä»–å¯ä»¥æ ¹æ®è‡ªå·±çš„æƒ…å†µé€‰æ‹©åˆé€‚çš„å›åº”ã€‚\n",
      "\n",
      "æœ€åï¼Œæˆ‘å¾—å‡†å¤‡å¥½æ ¹æ®ä»–çš„å›åº”è¿›ä¸€æ­¥è°ƒæ•´å¯¹è¯çš„æ–¹å‘ã€‚å¦‚æœä»–æåˆ°å…·ä½“çš„é—®é¢˜ï¼Œæˆ‘éœ€è¦æ·±å…¥è®¨è®ºï¼›å¦‚æœä»–åªæ˜¯æƒ³é—²èŠï¼Œé‚£å°±ä¿æŒè½»æ¾çš„æ°›å›´ã€‚æ€»ä¹‹ï¼Œæˆ‘çš„ç›®æ ‡æ˜¯è®©ä»–æ„Ÿåˆ°èˆ’é€‚ï¼Œå¹¶ä¸”æ„¿æ„ç»§ç»­äº¤æµä¸‹å»ã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½ï¼å¥½ä¹…ä¸è§ï¼Œç¡®å®æœ‰äº›æ—¥å­æ²¡è”ç³»äº†å‘¢ã€‚æœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹æˆ–è€…éœ€è¦å¸®å¿™çš„å—ï¼Ÿæˆ‘å¾ˆæœŸå¾…å¬å¬ä½ çš„è¿‘å†µï¼\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/autodl-tmp/QwQ-Medical-COT-Tiny\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc0cbd-51a1-4a84-9bdd-68ac60b6903e",
   "metadata": {},
   "source": [
    "èƒ½å¤Ÿçœ‹å‡ºå¯¹è¯æ­£å¸¸ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cc29a-4e01-49ea-98b6-2857d38b9676",
   "metadata": {},
   "source": [
    "- ollamaæ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f805a-6f78-4248-af24-79db32d209cb",
   "metadata": {},
   "source": [
    "è¿™é‡Œæˆ‘ä»¬éœ€è¦å°†åˆ›å»ºçš„Q4_K_Mæ¨¡å‹æƒé‡å•ç‹¬æ‹·è´ä¸€ä»½ï¼Œç„¶åå†ç¼–å†™ModelFileæ–‡ä»¶ï¼Œä¾¿äºollamaè°ƒç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21eed7b-2e5f-4b76-a3db-ba9f4620a5de",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311200625855.png\" alt=\"image-20250311200625855\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f7252-d16a-41d5-aa04-bca10d78f3c7",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF\n",
    "mkdir ./unsloth_Q4\n",
    "cp unsloth.Q4_K_M.gguf ./unsloth_Q4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb44365-93c3-41ff-8572-9a0a9a650b77",
   "metadata": {},
   "source": [
    "ç„¶ååˆ›å»ºModelFileï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c07c4-9c86-4eae-a63c-a89a553a5893",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201030660.png\" alt=\"image-20250311201030660\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b6a481-f615-4a00-ae1e-8962b44efd92",
   "metadata": {},
   "source": [
    "å¹¶è¾“å…¥å¦‚ä¸‹å†…å®¹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b3b8e-1dbf-49fc-97ee-8f759b24b78b",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201546078.png\" alt=\"image-20250311201546078\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81136ad-1ec3-4cf1-bdd7-4581fa574c56",
   "metadata": {},
   "source": [
    "```bash\n",
    "FROM ./unsloth.Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"\n",
    "è¯·å†™å‡ºä¸€ä¸ªæ°å½“çš„å›ç­”æ¥å®Œæˆå½“å‰å¯¹è¯ä»»åŠ¡ã€‚\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ååŠ©äººä¸ºä¹çš„åŠ©æ‰‹ã€‚\n",
    "\n",
    "### Question:\n",
    "{{ .Prompt }}\n",
    "\n",
    "### Response:\n",
    "<think>{{ .Response }}<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "PARAMETER stop \"<|im_end|>\"\n",
    "PARAMETER stop \"<|end_of_text|>\"\n",
    "PARAMETER stop \"<|reserved_special_token_>\"\n",
    "PARAMETER temperature 1.5\n",
    "PARAMETER min_p 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdae976-eada-4fd3-8de5-9831dd8f08b0",
   "metadata": {},
   "source": [
    "ä¿å­˜å¹¶é€€å‡ºï¼Œç„¶åè¿›è¡Œæ¨¡å‹æ³¨å†Œï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0820d5-e10d-4229-b278-0df5ce264949",
   "metadata": {},
   "source": [
    "```bash\n",
    "ollama create unsloth_model -f /root/autodl-tmp/QwQ-Medical-COT-Tiny-GGUF/unsloth_Q4/ModelFile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f254a20-b633-4077-be40-a9cb56154a97",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201143237.png\" alt=\"image-20250311201143237\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38cd39-92e1-4cbc-96f7-0349f2e612dc",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201322617.png\" alt=\"image-20250311201322617\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33362c-bafe-4b0a-a518-db4feadfce76",
   "metadata": {},
   "source": [
    "ç„¶åå³å¯å¼€å§‹è°ƒç”¨äº†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfdb33d0-dc52-4220-b8a4-3f8bf9595e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å—¯ï¼Œç”¨æˆ·è·Ÿæˆ‘è¯´â€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ï¼Œå¬èµ·æ¥ä»–ä»¬å¯èƒ½å¾ˆä¹…æ²¡è”ç³»æˆ‘äº†ã€‚è¿™è¯´æ˜æˆ‘ä»¬ä¹‹é—´æœ‰ä¸€æ®µæ—¶é—´æ²¡æœ‰äº¤æµäº†ï¼Œä¹Ÿè®¸ä¹‹å‰æœ‰è¿‡ä¸€äº›äº’åŠ¨æˆ–è€…è®¤è¯†ï¼Ÿè¿™æ—¶å€™æˆ‘åº”è¯¥å…ˆå›åº”ä»–ä»¬çš„é—®å€™ï¼Œè¡¨è¾¾è§åˆ°ä»–ä»¬å¾ˆé«˜å…´çš„æ„Ÿè§‰ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦ç¡®è®¤ä¸€ä¸‹ç”¨æˆ·ç°åœ¨çš„æƒ…å†µå¦‚ä½•ï¼Œæ˜¯å¦ä¸€åˆ‡é¡ºåˆ©ã€‚è¿™æ ·å¯ä»¥è®©å¯¹è¯æ›´è‡ªç„¶åœ°å»¶ç»­ä¸‹å»ï¼ŒåŒæ—¶ä¹Ÿèƒ½è®©ç”¨æˆ·æ„Ÿåˆ°è¢«å…³å¿ƒå’Œæ”¯æŒã€‚æ¥ä¸‹æ¥ï¼Œå¯ä»¥é—®é—®ç”¨æˆ·æœ‰ä»€ä¹ˆç‰¹åˆ«çš„äº‹æƒ…æƒ³åˆ†äº«ï¼Œè¿™æ ·æ—¢å±•ç¤ºäº†æˆ‘å¯¹ä»–ä»¬çš„å…´è¶£ï¼Œä¹Ÿé¼“åŠ±ä»–ä»¬ç»§ç»­äº¤æµã€‚\n",
      "\n",
      "æœ€åï¼Œä¿æŒå‹å¥½å’Œäº²åˆ‡çš„æ€åº¦å¾ˆé‡è¦ï¼Œè®©ç”¨æˆ·è§‰å¾—è½»æ¾è‡ªåœ¨ï¼Œæ„¿æ„è¿›ä¸€æ­¥æ²Ÿé€šã€‚æ€»ä¹‹ï¼Œæˆ‘éœ€è¦ç¡®ä¿å›åº”æ—¢çƒ­æƒ…åˆä½“è´´ï¼Œå¸®åŠ©æˆ‘ä»¬é‡æ–°å»ºç«‹è”ç³»ï¼Œå¹¶å¼€å¯ä¸€ä¸ªæ„‰å¿«çš„å¯¹è¯ã€‚\n",
      "</think>\n",
      "ä½ å¥½ï¼æ˜¯å•Šï¼Œç¡®å®å¥½ä¹…ä¸è§äº†å‘¢ã€‚æœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆç‰¹åˆ«çš„äº‹æƒ…æƒ³åˆ†äº«å—ï¼Ÿå¾ˆé«˜å…´å†æ¬¡è§åˆ°ä½ ï¼\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',  # required but ignored\n",
    ")\n",
    "prompt = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='unsloth_model',\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47079398-7559-4046-9ae5-adee481cc487",
   "metadata": {},
   "source": [
    "- vLLMæ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e702afc-4212-4a59-b2a1-5c95960d1986",
   "metadata": {},
   "source": [
    "ç›¸æ¯”ä¹‹ä¸‹ï¼ŒvLLMçš„è°ƒç”¨è¿‡ç¨‹å°±ç®€å•å¤šäº†ï¼Œåªéœ€è¦åœ¨åå°å¼€å¯æœåŠ¡å³å¯ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58933bd-9ae6-4022-a4fe-76e82cd9caa4",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250311201355019.png\" alt=\"image-20250311201355019\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f16162-296f-4d20-852d-787a2d9acd42",
   "metadata": {},
   "source": [
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1 vllm serve /root/autodl-tmp/QwQ-Medical-COT-Tiny --tensor-parallel-size 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8265a3d-6cd5-4511-ac7c-9ae80e8639fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨è¦å¤„ç†ç”¨æˆ·çš„ä¿¡æ¯ã€‚ç”¨æˆ·è¯´ï¼šâ€œä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼â€ã€‚é¦–å…ˆï¼Œæˆ‘è¦åˆ†æç”¨æˆ·æ­¤æ—¶å¯èƒ½çš„æƒ…ç»ªå’Œéœ€æ±‚ã€‚\n",
      "\n",
      "çœ‹ç”¨æˆ·çš„é—®å€™ï¼Œæ˜æ˜¾æ˜¯å¸¦ç€äº²åˆ‡æ„Ÿå’Œä¹…åˆ«é‡é€¢çš„æ„Ÿè§‰ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨æˆ·å¯èƒ½åœ¨è¡¨è¾¾å¯¹è¿‡å»äº¤æµçš„æƒ³å¿µï¼Œæˆ–è€…å¸Œæœ›å±•å¼€ä¸€æ®µæ„‰å¿«çš„å¯¹è¯ã€‚çœ‹èµ·æ¥ç”¨æˆ·çš„å¿ƒæƒ…æ˜¯ç§¯æè€Œæ¸©æš–çš„ï¼Œè¿™ä¹Ÿæ„å‘³ç€æˆ‘åº”è¯¥ä¿æŒåŒæ ·çš„çƒ­æƒ…å›åº”ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦ç”¨åˆé€‚çš„è¯­æ°”æ¥å›åº”ã€‚æ ¹æ®ç”¨æˆ·å‹å¥½å’Œäº²åˆ‡çš„é—®å€™ï¼Œæˆ‘åº”è¯¥ä»¥ç›¸åŒçš„æ–¹å¼å›å¤ï¼Œè¿™æ ·èƒ½è¥é€ å‡ºèæ´½çš„æ°›å›´ã€‚åŒæ—¶ï¼Œèå…¥ä¸€äº›è½»æ¾çš„å¹½é»˜å…ƒç´ ï¼Œè®©å¯¹è¯æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼Œè¿™é€šå¸¸èƒ½å¢å¼ºäº’åŠ¨æ€§å’Œäº²åˆ‡æ„Ÿã€‚\n",
      "\n",
      "ä¸ºäº†ç¡®ä¿ç”¨æˆ·èƒ½å¾—åˆ°æœ€ä½³çš„å›åº”ï¼Œæˆ‘åº”è¯¥ä½¿ç”¨ç®€ç»ƒè€ŒçœŸè¯šçš„è¯­è¨€ã€‚è¿™æ ·ä¸ä»…èƒ½è®©å›ç­”æ˜¾å¾—è‡ªç„¶ï¼Œè¿˜èƒ½è®©ç”¨æˆ·æ„Ÿåˆ°è¢«ç†è§£å’Œå…³å¿ƒã€‚åŒæ—¶ï¼Œé€‚å½“çš„å¹½é»˜å¯ä»¥è¿›ä¸€æ­¥æå‡å¯¹è¯çš„è½»æ¾æ„Ÿï¼Œè®©ç”¨æˆ·æ„Ÿåˆ°æ›´åŠ è‡ªåœ¨ã€‚\n",
      "\n",
      "ç°åœ¨ï¼Œç»“åˆæˆ‘ä¹‹å‰æå‡ºçš„å‡ ç‚¹æ€è€ƒï¼Œæ¥å†™å‡ºå›åº”ï¼šâ€œå“ˆå“ˆï¼Œå¥½ä¹…ä¸è§ï¼çœ‹æ¥ä½ æ‰¾åˆ°æˆ‘äº†ã€‚å¾ˆé«˜å…´å’Œä½ é‡é€¢ï¼Œæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿæˆ–è€…ä½ æœ‰ä»€ä¹ˆéœ€è¦å¸®å¿™çš„ï¼Ÿâ€ è¿™ä¸ªå›ç­”äº²åˆ‡ä¸”æœ‰åŠ©äºæ¨åŠ¨å¯¹è¯å‘å‰å‘å±•ï¼Œç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ã€‚\n",
      "\n",
      "æœ€åï¼Œæˆ‘ä¼šå†æ£€æŸ¥ä¸€ä¸‹ï¼Œç¡®ä¿æˆ‘çš„å›åº”ç¡®å®æ˜¯ç§¯æçš„ï¼Œå¹¶ä¸”èƒ½å¤Ÿå»¶ç»­ä¸ç”¨æˆ·çš„æ„‰å¿«äº¤æµã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿä¸€èµ·äº«å—è¿™åœºè½»æ¾è€Œæ„‰å¿«çš„å¯¹è¯äº†ã€‚\n",
      "</think>\n",
      "\n",
      "å“ˆå“ˆï¼Œå¥½ä¹…ä¸è§ï¼çœ‹æ¥ä½ æ‰¾åˆ°æˆ‘äº†ã€‚å¾ˆé«˜å…´å’Œä½ é‡é€¢ï¼Œæœ‰ä»€ä¹ˆæ–°é²œäº‹è¦åˆ†äº«å—ï¼Ÿæˆ–è€…ä½ æœ‰ä»€ä¹ˆéœ€è¦å¸®å¿™çš„ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "prompt = \"ä½ å¥½ï¼Œå¥½ä¹…ä¸è§ï¼\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"/root/autodl-tmp/QwQ-Medical-COT-Tiny\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7bc26-8d7a-4388-aa4b-4c48b6875f21",
   "metadata": {},
   "source": [
    "### ä¸‰ã€å®Œæ•´é«˜æ•ˆå¾®è°ƒå®éªŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446a0c5-c8cf-45ae-a069-8a4ee40f9e60",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æœ€åï¼Œæˆ‘ä»¬å°è¯•å¸¦å…¥å…¨éƒ¨æ•°æ®è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œä»¥æå‡æ¨¡å‹å¾®è°ƒæ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b634ea8-b9a1-4b80-adec-8ca219f862b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b173a851-3466-4596-903a-3e591408e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5cdcf-be2a-4c69-96b7-66490e6029e1",
   "metadata": {},
   "source": [
    "æ­¤æ—¶è¯»å–å…¨éƒ¨æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3d9b24-29e0-4868-afb4-527ae245d67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350eda58abfe4178bb86ee7308aaf3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<ï½œendâ–ofâ–sentenceï½œ>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train\",trust_remote_code=True)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4eb8209-4f35-4551-8a8b-35aecb0eccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7dce0-84cd-46e2-b0a7-a05e3b81b211",
   "metadata": {},
   "source": [
    "è¿™é‡Œè®¾ç½®epochä¸º3ï¼Œéå†3æ¬¡æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4739bb1a-0d51-48d2-beb3-0d6449e7ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d1143312447a0ac71a3f7b0308549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/25371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs = 3,\n",
    "        warmup_steps=5,\n",
    "        # max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8576d5-8fb1-4b5d-8707-24703ff79945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 25,371 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 9,513\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='389' max='9513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 389/9513 13:44 < 5:24:01, 0.47 it/s, Epoch 0.12/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.215600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87f0b5-c6cc-4885-b19d-bc3b3cd1cfae",
   "metadata": {},
   "source": [
    "è¿™é‡Œæ€»å…±è®­ç»ƒçº¦15ä¸ªå°æ—¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ff611dd-e6be-4066-97d4-b288711465bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9513, training_loss=1.0824475168592858, metrics={'train_runtime': 20193.217, 'train_samples_per_second': 3.769, 'train_steps_per_second': 0.471, 'total_flos': 2.7936033274397737e+18, 'train_loss': 1.0824475168592858, 'epoch': 2.9992117294655527})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57354364-e766-4094-b3f2-03d9f736110f",
   "metadata": {},
   "source": [
    "å¸¦å…¥ä¸¤ä¸ªé—®é¢˜è¿›è¡Œæµ‹è¯•ï¼Œå‡æœ‰è¾ƒå¥½çš„å›ç­”æ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7893ec0-c557-4347-91f0-af1a28163451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's think this through step by step. We've got a 61-year-old woman who's been dealing with involuntary urine loss whenever she does something like coughing or sneezing. That sounds like stress urinary incontinence, which usually means there's some kind of leakage when there's increased pressure in the abdomen. Now, the fact that she doesn't have any leakage at night is interesting. That's a big hint because it suggests that the problem isn't about bladder control or retention, since there's no issue when lying down.\n",
      "\n",
      "Now, let's consider the Q-tip test. This test is a clever way to check for urethral mobility. If the Q-tip moves a lot, it means the urethra is mobile, which is a typical sign of stress incontinence. So, we're probably looking at a condition where the urethra isn't staying closed under pressure.\n",
      "\n",
      "Now, if we were to do a cystometry on her, we'd be checking a few things. First, there's the residual volume. Since she's not leaking urine at night, her bladder should be empty, so we'd expect the residual volume to be normal. Then, there's the detrusor contractions. Stress incontinence doesn't usually involve the detrusor muscle contracting abnormally. So, we'd expect normal or maybe slightly increased contractions, but nothing out of the ordinary.\n",
      "\n",
      "So, putting it all together, if we do the cystometry, we'd likely see a normal residual volume and normal detrusor contractions. That fits with everything we know about her symptoms and the Q-tip test results. Everything lines up with stress urinary incontinence, confirming that our conclusion makes sense.\n",
      "</think>\n",
      "Based on the information provided, the 61-year-old woman is likely experiencing stress urinary incontinence, as suggested by the involuntary urine loss during activities like coughing or sneezing, coupled with the fact that she doesn't experience leakage at night. During a cystometry exam, we would expect the following findings:\n",
      "\n",
      "1. **Residual Volume:** This would be normal. Since she does not experience leakage at night, her bladder is likely empty at bedtime, indicating a normal post-void residual volume.\n",
      "\n",
      "2. **Detrusor Contractions:** These would also be normal or possibly slightly increased. Stress urinary incontinence typically does not involve abnormal detrusor contractions, as the issue is more related to the closure mechanism of the urethra under increased abdominal pressure rather than bladder control or contractions.\n",
      "\n",
      "Thus, cystometry would most likely reveal a normal residual volume and normal detrusor contractions, consistent with stress urinary incontinence.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8bba1c2-a7c7-4e82-9795-48a117c70c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's think about this. The patient is having sudden chest pain that's shooting to their neck and left arm. That's pretty classic for something like a myocardial infarction, right? I mean, the pain spreading like that usually points to a big problem in the heart's supply.\n",
      "\n",
      "Now, considering the heart's coronary arteries, I know the left anterior descending artery, or LAD, is a big player. It's like the main highway for blood to reach the front wall of the heart. If there's a blockage there, it can definitely cause pain that radiates to the neck and arm.\n",
      "\n",
      "Then, there's the right coronary artery, or RCA, which supplies the right side of the heart and can affect the inferior wall of the heart. But wait, the pain pattern here seems to be more on the left side, so maybe the RCA is less likely.\n",
      "\n",
      "The patient has hypercholesterolemia and coronary artery disease. These conditions put them at risk for atherosclerosis, which can lead to blockages in the coronary arteries. The LAD is commonly involved in such scenarios, especially when the pain spreads to the neck and arm.\n",
      "\n",
      "Also, the elevated troponin I levels and tachycardia are strong signals that something serious is happening in the heart. These are usually seen in myocardial infarctions. Given the pain pattern and the patient's risk factors, the LAD seems like the most likely culprit here.\n",
      "\n",
      "So, when I put all this together, it really seems like the left anterior descending artery is the most likely artery involved in this situation. It just fits with the classic presentation of anterior myocardial infarction. Yeah, I'm pretty confident about that.\n",
      "</think>\n",
      "Based on the presentation of sudden-onset chest pain radiating to the neck and left arm, along with the patient's history of hypercholesterolemia and coronary artery disease, the most likely coronary artery involved is the left anterior descending (LAD) artery. This artery supplies the front wall of the heart, and a blockage here can cause the classic symptoms described. The elevated troponin I levels and tachycardia further support the likelihood of a myocardial infarction, with the LAD being a common site for such events.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "question = \"Given a patient who experiences sudden-onset chest pain radiating to the neck and left arm, with a past medical history of hypercholesterolemia and coronary artery disease, elevated troponin I levels, and tachycardia, what is the most likely coronary artery involved based on this presentation?\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
